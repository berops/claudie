syntax = "proto3";
package spec;

import "google/protobuf/any.proto";
import "google/protobuf/timestamp.proto";
import "spec/dns.proto";
import "spec/nodepool.proto";
import "spec/pass.proto";

option go_package = "github.com/berops/claudie/proto/pb/spec";

// Config holds data for a single manifest.
message Config {
  // version of the config.
  uint64 version = 1;
  // Config name - same as input manifest name.
  string name = 2;
  // Information related within a k8s context.
  KubernetesContext k8sCtx = 3;
  // Client defined manifest.
  Manifest manifest = 4;
  // Clusters parsed from the supplied manifest.
  map<string, ClusterState> clusters = 5;
}

message Manifest {
  enum State {
    Pending = 0;
    Scheduled = 1;
    Done = 2;
    Error = 3;
  }

  string raw = 1;
  bytes checksum = 2;
  bytes lastAppliedChecksum = 3;
  State state = 4;
}

message ClusterState {
  Clusters current = 1;
  Workflow state = 4;
  TaskEvent inFlight = 5;
}

message Clusters {
  K8scluster k8s = 1;
  LoadBalancers loadBalancers = 2;
}

message LoadBalancers {
  repeated LBcluster clusters = 1;
}

message KubernetesContext {
  // name represents the input manifest resource name in Kubernetes
  string name = 1;
  // namespace represents the input manifest resource namespace in Kubernetes
  string namespace = 2;
}

message Workflow {
  enum Status {
    // DONE indicates that the workflow for a scheduled [Task] has finished.
    DONE = 0;

    // ERROR indicates that an error occurred while processing the scheduled [Task]
    ERROR = 1;

    // IN_PROGRESS indicates that the [Task] is currently being worked on.
    IN_PROGRESS = 2;

    // WAIT_FOR_PICKUP indicates that new a new [Task] was created or moved to
    // the next stage to be worked on.
    WAIT_FOR_PICKUP = 3;
  }

  Status status = 2;
  // additional information describing the state.
  string description = 3;
}

// K8scluster represents a single kubernetes cluster specified in the manifest.
message K8scluster {
  // General info about the cluster.
  ClusterInfo clusterInfo = 1;
  // Network range for the VPN.
  string network = 2;
  // Kubeconfig of the cluster.
  string kubeconfig = 3;
  // Kubernetes version.
  string kubernetes = 4;
  // General information about a proxy used to build a K8s cluster.
  InstallationProxy installationProxy = 5;
}

// LBcluster represents a single load balancer cluster specified in the
// manifest.
message LBcluster {
  // General info about the cluster.
  ClusterInfo clusterInfo = 1;
  // Array of Load balancer roles.
  repeated Role roles = 2;
  // DNS information.
  DNS dns = 3;
  // Kubernetes cluster name of the cluster this load balancer is attached to.
  string targetedK8s = 4;
  // usedApiEndpoint signals which LB is actually used as the api endpoint.
  // Claudie manifest validation does not allow for multiple API endpoints
  // to be present, however. Internally claudie can deal with more loadbalancers
  // that have the API role, this is due to the creation of intermediate representation
  // which always add new infrastructure before making any further changes.
  // To recognize which of them is actually used this field was added.
  bool usedApiEndpoint = 5;
}

// ClusterInfo holds general information about the clusters.
message ClusterInfo {
  // Name of the cluster.
  string name = 1;
  // Random hash of the cluster.
  string hash = 2;
  // Array of node pools this cluster is made of.
  repeated NodePool nodePools = 5;
}

// InstallationProxy holds general information about a proxy used to build a K8s cluster.
message InstallationProxy {
  // Proxy installation mode.
  string mode = 1;
  // Proxy endpoint used to access the proxy.
  string endpoint = 2;
  // NoProxy is a comma-separated list of values that will be added to the default list of NoProxies used by Claudie.
  //
  // The default no proxy list is: 127.0.0.1/8,localhost,cluster.local,10.244.0.0/16,10.96.0.0/12"
  // Any values specified will be appended to the end of the default NoProxy list.
  // This field only has an effect if the Proxy is turned on.
  string noProxy = 3;
}

// Role represents a single loadbalancer role from the manifest.
message Role {
  message Settings {
    bool proxyProtocol = 1;
    bool stickySessions = 2;

    // required port for the envoy admin interface,
    // on change will issue restart of the envoy proxy.
    int32 envoy_admin_port = 3;
  }
  // Name of the role.
  string name = 1;
  // Protocol that load balancer uses to forward traffic. ["tcp", "udp"]
  string protocol = 2;
  // Port that load balancer will forward from.
  int32 port = 3;
  // Port that load balancer will forward to.
  int32 targetPort = 4;
  // Type of the role.
  RoleType roleType = 6;
  // Targeted nodes in Kubernetes clusters.
  repeated string targetPools = 7;
  // Additional settings for the role.
  Settings settings = 8;
}

// RoleType specifies the type of the role.
enum RoleType {
  // API server load balancer.
  ApiServer = 0;
  // Ingress load balancer.
  Ingress = 1;
}

enum Event {
  UNKNOWN = 0;
  CREATE = 1;
  UPDATE = 2;
  DELETE = 3;
}

message TaskEvent {
  string id = 1;
  google.protobuf.Timestamp timestamp = 2;
  Event event = 3;
  Task task = 4;
  string description = 5;

  // Pipeline stages of the task through which it has to pass
  // to be considered as done.
  repeated Stage pipeline = 8;

  // Index into the currently to be worked on pipeline stage.
  uint32 currentStage = 9;
}

enum ApiEndpointChangeState {
  // NoChange represents the 1st case - no change is needed as the LB cluster is currently
  // attached and the desired spec contains no changes.
  NoChange = 0;

  // AttachingLoadBalancer represents 2nd case - the K8s cluster previously
  // didn't have an LB cluster attached and the ports needed to communicate with the API server
  // were exposed. After attaching an LB cluster to the existing K8s cluster the ports
  // were closed and are no longer accessible, and thus we need to change the API endpoint.
  AttachingLoadBalancer = 1;

  // DetachingLoadBalancer represents 3rd. case - the K8s cluster had an existing
  // LB cluster attached but the new state removed the LB cluster and thus the API endpoint
  // needs to be changed back to one of the control nodes of the cluster.
  DetachingLoadBalancer = 2;

  // EndpointRenamed represents the 4th. case - the K8s cluster has an existing
  // LB cluster attached and also keeps it but the endpoint has changed in the desired state.
  EndpointRenamed = 3;

  // MoveEndpoint represents the 5th. case - the K8s cluster has an existing
  // LB cluster attached, but it will be switched to a different LB cluster
  // in the desired state.
  MoveEndpoint = 4;
}

// Unreachable are the nodes of the kubernetes cluster
// and all of its loadbalancer clusters which are
// unreachable. If this is included in some message, one
// should omit connecting to those nodes during the processing
// of the pipeline, if relevant.
message Unreachable {
  message ListOfNodeEndpoints {
    repeated string endpoints = 1;
  }
  message UnreachableNodePools {
    map<string, ListOfNodeEndpoints> nodepools = 1;
  }

  UnreachableNodePools kubernetes = 1;
  map<string, UnreachableNodePools> loadbalancers = 2;
}

// Creates a new kubernetes cluster with the attached [LBcluster], if any.
message Create {
  // The desired state of the kuberenetes cluster to create.
  //
  // This field is required to be present.
  K8scluster k8s = 1;

  // The desired state of the LoadBalancers attached to the
  // kubernetes cluster to create.
  //
  // This field is optional, if the kuberentes cluster does not have
  // any loadbalacners.
  repeated LBcluster loadBalancers = 2;
}

message Update {
  // State is the current state as known by the Manager service that is continiously
  // updated and merged into the [Update] message as it traverses the build pipeline.
  message State {
    // The state of the kubernetes cluster to update.
    K8scluster k8s = 1;
    // The state of the loadbalancers to update.
    repeated LBcluster loadBalancers = 2;
  }

  message None {}

  // TerraformerAddLoadBalancer is a message that once processed
  // by the Terraformer service and a result is send back to the
  // Manager, the manager will consume the message further stages
  // will only have a [AddedLoadBalancer] message to process.
  //
  // New, "desired", state needs only to be present in the terraformer
  // stage and all other services need to only see what has been done
  // and ocasionally index the loadbalancer, which will already be present
  // in the [State] of the [Update] message.
  message TerraformerAddLoadBalancer {
    LBcluster handle = 1;
  }
  message AddedLoadBalancer {
    string handle = 1;
  }

  // TerraformerDeleteLoadBalancerNodes is a message that is processed by the
  // Terraformer service and a result is send back to the Manager, it will consume
  // the message and further stages will only have a [DeletedLoadBalancerNodes]
  // message to process.
  //
  // Old, "current", state needs to only be present in the Terraformer stage and
  // all other services need to only see the changes after the deletion for
  // reconciliation which will already be present in the [State] of the [Update]
  // message.
  message TerraformerDeleteLoadBalancerNodes {
    // Id of the loadbalancer.
    string handle = 1;

    // Whether the NodePool itself
    // was deleted from the cluster.
    // Meaning that all nodes of the
    // nodepool were deleted.
    bool withNodePool = 2;

    // Affected Nodepool.
    string nodepool = 3;

    // Nodes that were deleted.
    repeated string nodes = 4;

    // Unreachable infrastructure that is optionally set
    // for the deletion process, in which presense, connecting
    // to the unreachable infrastructure should be omitted.
    optional Unreachable unreachable = 5;
  }
  message DeletedLoadBalancerNodes {
    message WholeNodePool {
      // The nodepool that was deleted from the current
      // state along with all of its nodes.
      NodePool nodepool = 1;
    }
    message Partial {
      // The nodepool which was modified with the
      // deletion of the nodes, which is still present
      // in the [State] passed along the [Update] msg.
      string nodepool = 1;

      // The nodes that were deleted.
      repeated Node nodes = 2;

      // Map of keys for each node.
      // Will only be set if the partially
      // deleted nodes were part of a static
      // nodepool.
      // [<Node Endpoint>]<Key>.
      map<string, string> staticNodeKeys = 3;
    }

    // Unreachable infrastructure that is optionally set
    // for the deletion process, in which presense, connecting
    // to the unreachable infrastructure should be omitted.
    optional Unreachable unreachable = 1;

    // id of the loadbalancer.
    string handle = 2;

    oneof Kind {
      WholeNodePool whole = 3;
      Partial partial = 4;
    }
  }

  // TerraformerAddLoadBalancerNodes is a message that is processed by the
  // Terraformer service and a result is send back to the Manager, it will
  // consume the message and further stages will only have a [AddedLoadBalancerNodes]
  // message to process.
  //
  // New, "desired", state needs only to be present in the Terraformer stage and
  // all other services need to only see what has been done which will already
  // be present in the [State] of the [Update] message.
  message TerraformerAddLoadBalancerNodes {
    message Existing {
      // The nodepool which is to be modified with the
      // addition of new nodes, which is already present
      // in the [State] passed along the [Update] msg.
      string nodepool = 1;

      // Nodes to be added.
      repeated Node nodes = 2;
    }

    message New {
      // The new nodepool which is to be added to the
      // [State] passed along the [Update] msg.
      NodePool nodepool = 1;
    }

    // Id of the loadbalancer.
    string handle = 1;

    oneof Kind {
      Existing existing = 2;
      New new = 3;
    }
  }
  message AddedLoadBalancerNodes {
    string handle = 1;
    bool newNodePool = 2;
    string nodePool = 3;
    repeated string nodes = 4;
  }

  // DeleteLoadBalnacerRoles works with the [State] that is part of
  // the [Update] message, thus no "message consuming" needs to happen.
  message DeleteLoadBalancerRoles {
    string handle = 1;
    repeated string roles = 2;
  }

  // TerraformerAddLoadBalancerRoles is a message that is processed by the
  // Terraformer service and a result is send back to the Manager, it will
  // consume the message and further stages will only have a [AddedLoadBalancerRoles]
  // message to process.
  //
  // New, "desired", state needs only to be present in the Terraformer stage and
  // all other services need to only see what has been done which will already
  // be present in the [State] of the [Update] message.
  message TerraformerAddLoadBalancerRoles {
    string handle = 1;
    repeated Role roles = 2;
  }
  message AddedLoadBalancerRoles {
    string handle = 1;
    repeated string roles = 2;
  }

  // TerraformerReplaceDns is a message that once processed by the
  // Terraformer service and a result is send back to the Manager,
  // it will consume the message and further stages will only have
  // a [ReplacedDns] message to process.
  //
  // New, "desired", state needs only to be present in the terraformer
  // stage and all other services need to only see what has been done and
  // occasionally index the LoadBalancer, which will already be present
  // in the [State] of the [Update] message.
  message TerraformerReplaceDns {
    // Loadbalancer ID to which the new [Dns] is going to created for.
    string handle = 1;

    // The new dns to replace the old within [LoadBalancerId].
    DNS dns = 2;

    // If the [Dns] is part of a Api loadbalancer this
    // field will be set to give extra information for
    // when changing the Api endpoint from the old to
    // the new.
    optional string oldApiEndpoint = 3;
  }
  message ReplacedDns {
    string handle = 1;
    optional string oldApiEndpoint = 2;
  }

  // DeleteLoadBalancer works with the [State] that is part of the [Update]
  // message, thus no "message consuming" needs to happen.
  message DeleteLoadBalancer {
    string handle = 1;

    // Unreachable infrastructure that is optionally set
    // for the deletion process, in which presense, connecting
    // to the unreachable infrastructure should be omitted.
    optional Unreachable unreachable = 2;
  }

  // ApiEndpoint works with the [State] that is part of the [Update] message
  // thus no "message consuming" needs to happen.
  message ApiEndpoint {
    ApiEndpointChangeState state = 1;
    string currentEndpointId = 2;
    string desiredEndpointId = 3;
  }

  // K8sOnlyApiEndpoint works with the [State] that is part of the [Update]
  // message, thus no "message consuming" needs to happen.
  message K8sOnlyApiEndpoint {
    string nodepool = 1;
    string node = 2;
  }

  // ApiPortOnCluster works with the [State] that is part of the [Update] message
  // thus no "message consuming" needs to happen.
  message ApiPortOnCluster {
    bool open = 1;
  }

  // AnsiblerReplaceProxySettings is a message that once processed
  // by the Ansibler service a result is send back to the Manager,
  // it will consume the message and further stages will only have
  // a [ReplacedProxySettings] message to process.
  //
  // New, "desired", state needs only to be present in the ansibler
  // stage and all other services need to only see what has been done
  // which will already be present in the [State] of the [Update] message.
  message AnsiblerReplaceProxySettings {
    InstallationProxy proxy = 1;
  }
  message ReplacedProxySettings {}

  // AnsiblerReplaceTargetPools is a message that once processed
  // by the Ansibler service a result is send back to the Manager,
  // it will consume the message and further stages will only have
  // a [ReplacedTargetPools] message to process.
  //
  // New, "desired", state needs only to be present in the ansibler
  // stage and all other services need to only see what has been done
  // which will already be present in the [State] of the [Update] message.
  message AnsiblerReplaceTargetPools {
    message TargetPools {
      repeated string pools = 1;
    }
    string handle = 1;

    // Role to target pools
    map<string, TargetPools> roles = 2;
  }
  message ReplacedTargetPools {
    message TargetPools {
      repeated string pools = 1;
    }
    string handle = 1;

    // Role to target pools.
    map<string, TargetPools> roles = 2;
  }

  // UpgradeVersion works with the [State] that is part of the [Update]
  // message, thus no "message consuming" needs to happen.
  message UpgradeVersion {
    string version = 1;
  }

  // KuberPatchNodes is a message that once processed by the Kuber service
  // and a result is send back to the Manager, it will consume the message
  // and further stages will only have a [PatchedNodes] message to process.
  //
  // New, "desired", state needs only to be present in the kuber stage and
  // all other services need to only see what has been done which will already
  // be present in the [State] of the [Update] message.
  message KuberPatchNodes {
    message ListOfTaints {
      repeated spec.Taint taints = 1;
    }
    message ListOfLabelKeys {
      repeated string labels = 1;
    }
    message ListOfAnnotationKeys {
      repeated string annotations = 1;
    }
    message MapOfLabels {
      map<string, string> labels = 1;
    }
    message MapOfAnnotations {
      map<string, string> annotations = 1;
    }
    message RemoveBatch {
      map<string, ListOfTaints> taints = 1;
      map<string, ListOfAnnotationKeys> annotations = 2;
      map<string, ListOfLabelKeys> labels = 3;
    }
    message AddBatch {
      map<string, ListOfTaints> taints = 1;
      map<string, MapOfLabels> labels = 2;
      map<string, MapOfAnnotations> annotations = 3;
    }

    AddBatch add = 1;
    RemoveBatch remove = 2;
  }
  message PatchedNodes {}

  // KuberDeleteK8sNodes is a message that is processed by the Kuber service
  // and a result is send back to the Manager, it will consume the message
  // and further stages will only have a [DeletedK8sNodes] message to process.
  //
  // Old, "current", state needs to only be present in the Kuber stage and
  // all other services need to only see the changes after the deletion for
  // reconciliation which will already be present in the [State] of the [Update]
  // message.
  message KuberDeleteK8sNodes {
    // Whether the NodePool itself
    // was deleted from the cluster.
    // Meaning that all nodes of the
    // nodepool were deleted.
    //
    // Note that with autoscaling its
    // possible to have a zero sized
    // nodepool so this is only set
    // if the nodepool is removed from
    // the cluster.
    bool withNodePool = 1;

    // Affected Nodepool.
    string nodepool = 2;

    // Nodes that were deleted.
    repeated string nodes = 3;

    // Unreachable infrastructure that is optionally set
    // for the deletion process, in which presense, connecting
    // to the unreachable infrastructure should be omitted.
    optional Unreachable unreachable = 4;
  }
  message DeletedK8sNodes {
    message WholeNodePool {
      // The nodepool that was deleted from the current
      // state along with all of its nodes.
      NodePool nodepool = 1;
    }
    message Partial {
      // The nodepool which was modified with the
      // deletion of the nodes, which is still present
      // in the [State] passed along the [Update] msg.
      string nodepool = 1;

      // Nodes that were deleted.
      repeated Node nodes = 2;

      // Map of keys for each node.
      // Will only be set if the partially
      // deleted nodes were part of a static
      // nodepool.
      // [<Node Endpoint>]<Key>.
      map<string, string> staticNodeKeys = 3;
    }

    // Unreachable infrastructure that is optionally set
    // for the deletion process, in which presense, connecting
    // to the unreachable infrastructure should be omitted.
    optional Unreachable unreachable = 1;

    oneof Kind {
      WholeNodePool whole = 2;
      Partial partial = 3;
    }
  }

  // TerraformerAddK8sNodes is a message that is processed by the Terraformer service
  // and a result is send back to the Manager, it will consume the message
  // and further stages will only have a [AddedK8sNodes] message to process.
  //
  // New, "desired", state needs only to be present in the Terraformer stage and
  // all other services need to only see what has been done which will already
  // be present in the [State] of the [Update] message.
  message TerraformerAddK8sNodes {
    message Existing {
      // The nodepool which is to be modified with the
      // addition of new nodes, which is already present
      // in the [State] passed along the [Update] msg.
      string nodepool = 1;

      // Nodes to be added.
      repeated Node nodes = 2;
    }
    message New {
      // The new nodepool which is to be added to the
      // [State] passed along the [Update] msg.
      NodePool nodepool = 1;
    }

    oneof Kind {
      Existing existing = 1;
      New new = 2;
    }
  }
  message AddedK8sNodes {
    bool newNodePool = 1;
    string nodepool = 2;
    repeated string nodes = 3;
  }

  // State that gets continiously updated as the task gets processed.
  State state = 1;

  // Additional information while the [Update] message is passed through
  // the build pipeline to give context what has been scheduled and what
  // to process.
  oneof Delta {
    None none = 2;

    // Terraformer messages to consume
    TerraformerAddLoadBalancer tfAddLoadBalancer = 3;
    TerraformerAddLoadBalancerNodes tfAddLoadBalancerNodes = 4;
    TerraformerReplaceDns tfReplaceDns = 5;
    TerraformerAddK8sNodes tfAddK8sNodes = 6;
    TerraformerAddLoadBalancerRoles tfAddLoadBalancerRoles = 7;
    TerraformerDeleteLoadBalancerNodes tfDeleteLoadBalancerNodes = 8;

    // Ansibler message to consume
    AnsiblerReplaceProxySettings ansReplaceProxy = 10;
    AnsiblerReplaceTargetPools ansReplaceTargetPools = 11;

    // Kuber messages to consume
    KuberPatchNodes kpatchNodes = 13;
    KuberDeleteK8sNodes kDeleteNodes = 14;

    // Consumed messages.
    AddedLoadBalancer addedLoadBalancer = 15;
    AddedLoadBalancerNodes addedLoadBalancerNodes = 16;
    ReplacedDns replacedDns = 17;
    AddedK8sNodes addedK8sNodes = 18;
    ReplacedProxySettings replacedProxy = 19;
    PatchedNodes patchedNodes = 20;
    AddedLoadBalancerRoles addedLoadBalancerRoles = 21;
    ReplacedTargetPools replacedTargetPools = 22;

    // Deletions.
    DeleteLoadBalancer deleteLoadBalancer = 23;
    DeletedK8sNodes deletedK8sNodes = 24;
    DeletedLoadBalancerNodes deletedLoadBalancerNodes = 25;
    DeleteLoadBalancerRoles deleteLoadBalancerRoles = 26;

    // Utilities.
    ApiEndpoint apiEndpoint = 27;
    ApiPortOnCluster clusterApiPort = 28;
    K8sOnlyApiEndpoint k8sApiEndpoint = 29;
    UpgradeVersion upgradeVersion = 30;
  }
}

// Deletes an existing kubernetes cluster along with its attached [LBcluster], if any.
message Delete {
  // The current state of the kuberentes cluster to delete.
  //
  // This field is required
  K8scluster k8s = 1;

  // The current state of the loadbalancers  to delete, that
  // are attached to the kubernetes cluster.
  //
  // This field depends on the above [k8s] field and is required if
  // the kuberentes cluster has any loadbalancers attached.
  repeated LBcluster loadBalancers = 2;
}

message Task {
  oneof Do {
    Create create = 7;
    Update update = 8;
    Delete delete = 9;
  }
}

// The message that is used for the Message Queue.
message Work {
  // The task to be worked on.
  Task task = 1;

  // What exactly should be done with the task.
  repeated google.protobuf.Any passes = 2;
}

message TaskResult {
  message Error {
    enum Kind {
      FATAL = 0;
      PARTIAL = 1;
    }

    Kind kind = 1;
    string description = 2;
  }

  // None specifies that nothing should be done as a result of
  // processing a task other than acknowledging that the task
  // was processed, and thus moving to next task, if any.
  message None {}

  // UpdateState specifies the current state should be updated
  // as changes may have been done to it. All of the values
  // are optionals and the receiving end should make sense of
  // them.
  // Any non-nil value signals an update that should be handled.
  message UpdateState {
    // If the [k8s] field is set then its current state should be merged/replaced
    // with the state in this message.
    optional K8scluster k8s = 1;

    // Updates the loadbalancers that are attached to the [k8s] cluster.
    // The loadbalancers in this message should only update existing matching
    // loadbalancers and or add newly created ones.
    //
    // If any loadbalancers are missing while they're present in the current state
    // they should not be deleted, as for that there is an explicit task result [ClearState].
    optional LoadBalancers loadBalancers = 2;
  }

  // ClearState specifies cluster IDs which should be cleared as changes
  // processed by a scheduled task destroyed the infrastructure.
  // Any non-nil value signals an update that should be done.
  message ClearState {
    // If the kuberentes cluster is specified than all of its loadbalancers
    // should be deleted even if they're not specified within this message.
    optional bool k8s = 1;

    // ID of the loadbalancers that should be cleared.
    repeated string loadBalancersIDs = 2;
  }

  // If the Error.Kind is FATAL no useful
  // information is stored in the result.
  // Otherwise, if error is not set or PARTIAL,
  // the result will contain valid data,.
  optional Error error = 1;

  oneof Result {
    None none = 2;
    UpdateState update = 3;
    ClearState clear = 4;
  }
}
