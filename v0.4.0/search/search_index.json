{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Docs Overview","text":"<p>In case you are reading our documentation in our GitHub repository, you might want to read the prettier version of it on docs.claudie.io</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The \"Getting Started\" section is where you'll learn how to begin using Claudie. We'll guide you through the initial steps and show you how to set things up, so you can start using the software right away. </p> <p>You'll also find helpful information on how to customize Claudie to suit your needs, including specifications for the settings you can adjust, and examples of how to use configuration files to get started.</p> <p>By following the steps in this section, you'll have everything you need to start using Claudie with confidence!</p>"},{"location":"#how-claudie-works","title":"How Claudie works","text":"<p>In this section, we'll show you how Claudie works and guide you through our workflow. We'll explain how we store and manage data, balance the workload across different parts of the system, and automatically adjust resources to handle changes in demand.</p> <p>By following our explanations, you'll gain a better understanding of how Claudie operates and be better equipped to use it effectively.</p>"},{"location":"#claudie-use-cases","title":"Claudie Use Cases","text":"<p>The \"Claudie Use Cases\" section includes examples of different ways you can use Claudie to solve various problems. We've included these examples to help you understand the full range of capabilities Claudie offers and to show you how it can be applied in different scenarios. </p> <p>By exploring these use cases, you'll get a better sense of how Claudie can be a valuable tool for your work.</p>"},{"location":"#faq","title":"FAQ","text":"<p>You may find helpful answers in our FAQ section.</p>"},{"location":"#roadmap-for-claudie","title":"Roadmap for Claudie","text":"<p>In this section, you'll find a roadmap for Claudie that outlines the features we've already added and those we plan to add in the future.</p> <p>By checking out the roadmap, you'll be able to stay informed about the latest updates and see how Claudie is evolving to meet the needs of its users.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>In this section, we've gathered all the information you'll need if you want to help contribute to the Claudie project or release a new version of the software. </p> <p>By checking out this section, you'll get a better sense of what's involved in contributing and how you can be part of making Claudie even better.</p>"},{"location":"#changelog","title":"Changelog","text":"<p>The \"changelog\" section is where you can find information about all the changes, updates, and issues related to each version of Claudie. </p>"},{"location":"CHANGELOG/changelog-0.1.x/","title":"Claudie <code>v0.1</code>","text":"<p>The first official release of Claudie</p>"},{"location":"CHANGELOG/changelog-0.1.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.1.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.1.x/#v013","title":"v0.1.3","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features","title":"Features","text":"<ul> <li>Change the workflow of the Claudie, to build the infrastructure on per cluster basis #584</li> <li>Add labels on Claudie created resources #579</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes","title":"Bugfixes","text":"<p>No bugfixes since the last release.</p>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues","title":"Known issues","text":"<ul> <li><code>k8s-sidecar</code> sometimes misses deletion of the input manifest secret #588</li> <li>Deleting nodes in Builder is not idempotent #587</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#v012","title":"v0.1.2","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features_1","title":"Features","text":"<ul> <li>Update to Go <code>v1.20</code> #559</li> <li>The VPN now respects netmask from defined CIDR #571</li> <li>Connections attempt are more readable #570</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes_1","title":"Bugfixes","text":"<ul> <li>Wireguard IP now persists across reboots #557</li> <li>Deletion of the infrastructure before any outputs were created does not end with error #569</li> <li>Replace the <code>azurerm_virtual_machine</code> to the <code>azurerm_linux_virtual_machine</code> #573</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues_1","title":"Known issues","text":"<ul> <li>Longhorn replicas are  not properly managed, which might cause issues when deleting nodes #564</li> <li>Naming scheme in input manifest is not uniform #563</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#v011","title":"v0.1.1","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features_2","title":"Features","text":"<ul> <li>Support DNS zone for Cloudflare, AWS, Azure, HetznerDNS, OCI #530</li> <li>Add default node labels #543</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes_2","title":"Bugfixes","text":"<ul> <li>Logs in all services have been modified to not output sensitive information. #535</li> <li>Correctly update desiredState after workflow for a given manifest completes. #536</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues_2","title":"Known issues","text":"<ul> <li>Wireguard interface <code>wg0</code> is missing ip address after reboot. Will be fixed in next release #557</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#v010","title":"v0.1.0","text":""},{"location":"CHANGELOG/changelog-0.1.x/#features_3","title":"Features","text":"<ul> <li>Multi-cloud kubernetes cluster management</li> <li>Multi-cloud loadbalancer management</li> <li>Fast scale-up/scale-down of defined infrastructure</li> <li>Persistent storage via Longhorn</li> <li>Support for AWS, Azure, GCP, OCI and Hetzner</li> <li>GCP DNS zone support</li> <li>Claudie deployment on <code>amd64</code> and <code>arm64</code> clusters</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#bugfixes_3","title":"Bugfixes","text":"<ul> <li>As this is first release there are no bugfixes</li> </ul>"},{"location":"CHANGELOG/changelog-0.1.x/#known-issues_3","title":"Known issues","text":"<ul> <li><code>iptables</code> reset after reboot and block all traffic on <code>OCI</code> node #466</li> <li>Occasional connection issues between Claudie created clusters and Claudie on Hetzner and GCP #276</li> <li>Unable to easily recover after error #528</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/","title":"Claudie <code>v0.2</code>","text":"<p>Due to a breaking change in the input manifest schema, the <code>v0.2.x</code> will not be backwards compatible with <code>v0.1.x</code>.</p>"},{"location":"CHANGELOG/changelog-0.2.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.2.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.2.x/#v020","title":"v0.2.0","text":""},{"location":"CHANGELOG/changelog-0.2.x/#features","title":"Features","text":"<ul> <li>Unify the naming schema in the input manifest #601</li> <li>Deploy MinIO in multi-replica fashion #589</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#bugfixes","title":"Bugfixes","text":"<p>No bugfixes since the last release.</p>"},{"location":"CHANGELOG/changelog-0.2.x/#known-issues","title":"Known issues","text":"<ul> <li>Workflow fails to build when a user makes multiple changes of the input manifest, regarding the API endpoint #606</li> <li>Longhorn pod longhorn-admission-webhook stuck in Init state #598</li> <li>Deletion of config fails if builder crashes after deleting nodes #588</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#v021","title":"v0.2.1","text":""},{"location":"CHANGELOG/changelog-0.2.x/#features_1","title":"Features","text":"<ul> <li>Improve management of Longhorn volume replicas #648</li> <li>Improve logging on all services #657</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#bugfixes_1","title":"Bugfixes","text":"<ul> <li>Fix unnecessary restarts in Wireguard playbook #658</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#known-issues_1","title":"Known issues","text":"<ul> <li>Certain change in nodepool configuration forces replacement of VMs #647</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#v022","title":"v0.2.2","text":""},{"location":"CHANGELOG/changelog-0.2.x/#features_2","title":"Features","text":"<ul> <li>Cluster Autoscaler integration #644</li> <li>Drop using grpc-health-probe in favour of Kubernetes native grpc health probes. #691</li> <li>Centralized information about the current workflow state of a cluster in the frontend service #605</li> </ul>"},{"location":"CHANGELOG/changelog-0.2.x/#bugfixes_2","title":"Bugfixes","text":"<ul> <li>Certain change in nodepool configuration forces replacement of VMs #647</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/","title":"Claudie <code>v0.3</code>","text":"<p>Due to a breaking change in the input manifest schema, the <code>v0.3.x</code> will not be backwards compatible with <code>v0.2.x</code></p>"},{"location":"CHANGELOG/changelog-0.3.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.3.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.3.x/#v030","title":"v0.3.0","text":""},{"location":"CHANGELOG/changelog-0.3.x/#features","title":"Features","text":"<ul> <li>Use separate storage disk for longhorn #689</li> <li>Apply proper kubernetes labels to Claudie resources #714</li> <li>Implement clean architecture for the Frontend #701</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#bugfixes","title":"Bugfixes","text":"<ul> <li>Fix logging issues in Frontend #713</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#known-issues","title":"Known issues","text":"<ul> <li>Infrastructure might not get deleted if workflow encounters and error #712</li> <li>Certain cluster manipulation can result in workflow failing to build the clusters #606</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#v031","title":"v0.3.1","text":""},{"location":"CHANGELOG/changelog-0.3.x/#features_1","title":"Features","text":"<ul> <li>Rework logs in all microservices to enable easier filtering #742</li> <li>Improve longhorn volume replication management #782</li> <li>Various improvements in cluster manipulation #728</li> <li>Removal of <code>k8s-sidecar</code> from Frontend #792</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#bugfixes_1","title":"Bugfixes","text":"<ul> <li>Fixed bug when infrastructure was not deleted if workflow encountered an error #773</li> <li>Fixed error when deletion of nodes from cluster failed #728</li> <li>Fixed bug when frontend triggered deletion of incorrect manifest #744</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#known-issues_1","title":"Known issues","text":"<ul> <li>Subnet CIDR is not carried over from temporary state in Builder #790</li> <li>Longhorn occasionally does not detach volume from node which was deleted #784</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#v032","title":"v0.3.2","text":""},{"location":"CHANGELOG/changelog-0.3.x/#features_2","title":"Features","text":"<ul> <li>Label Claudie output secrets #837</li> <li>Use cluster name in the output kubeconfig context #830</li> <li>Make DynamoDB job idempotent #817</li> <li>Implement secret validation webhook #821</li> <li>Improve Cluster Autoscaler deployment #805</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#bugfixes_2","title":"Bugfixes","text":"<ul> <li>Fixed bug when subnet CIDR is not carried over from temporary state in Builder #812</li> </ul>"},{"location":"CHANGELOG/changelog-0.3.x/#known-issues_2","title":"Known issues","text":"<p>No known issues since the last release.</p>"},{"location":"CHANGELOG/changelog-0.4.x/","title":"Claudie <code>v0.4</code>","text":"<p>Due to a breaking change in the input manifest schema, the <code>v0.4.x</code> will not be backwards compatible with <code>v0.3.x</code></p>"},{"location":"CHANGELOG/changelog-0.4.x/#deployment","title":"Deployment","text":"<p>To deploy the Claudie <code>v0.4.X</code>, please:</p> <ol> <li> <p>Download the archive and checksums from the release page</p> </li> <li> <p>Verify the archive with the <code>sha256</code> (optional)</p> <pre><code>sha256sum -c --ignore-missing checksums.txt\n</code></pre> <p>If valid, output is, depending on the archive downloaded</p> <pre><code>claudie.tar.gz: OK\n</code></pre> <p>or</p> <pre><code>claudie.zip: OK\n</code></pre> <p>or both.</p> </li> <li> <p>Lastly, unpack the archive and deploy using <code>kubectl</code></p> <p>We strongly recommend changing the default credentials for MongoDB, MinIO and DynamoDB before you deploy it. To do this, change contents of the files in <code>mongo/secrets</code>, <code>minio/secrets</code> and <code>dynamo/secrets</code> respectively.</p> <pre><code>kubectl apply -k .\n</code></pre> </li> </ol>"},{"location":"CHANGELOG/changelog-0.4.x/#v040","title":"v0.4.0","text":""},{"location":"CHANGELOG/changelog-0.4.x/#features","title":"Features","text":"<ul> <li>Input manifest definition now uses CRD instead of secret #872</li> <li>Various improvements in the overall documentation #864 #871 #884 #888 #891 #893</li> </ul>"},{"location":"CHANGELOG/changelog-0.4.x/#bugfixes","title":"Bugfixes","text":"<ul> <li>Errors from the Scheduler are correctly saved under the clusters state #868</li> <li>Failure in the Terraformer will correctly saves the created state #875</li> <li>The clusters which previously resulted in error no longer block the workflow on input manifest reapply #883</li> </ul>"},{"location":"CHANGELOG/changelog-0.4.x/#known-issues","title":"Known issues","text":"<ul> <li>Single node pool definition cannot be used as control plane and as compute plane in the same cluster. #865</li> <li>Input manifest status is not tracked during autoscaling #886</li> </ul>"},{"location":"autoscaling/autoscaling/","title":"Autoscaling in Claudie","text":"<p>Claudie supports autoscaling by installing Cluster Autoscaler for Claudie-made clusters, with a custom implementation of <code>external gRPC cloud provider</code>, in Claudie context called <code>autoscaler-adapter</code>. This, together with Cluster Autoscaler is automatically managed by Claudie, for any clusters, which have at least one node pool defined with <code>autoscaler</code> field. Whats more, you can change the node pool specification freely from autoscaler configuration to static count or vice versa. Claudie will seamlessly configure Cluster Autoscaler, or even remove it when it is no longer needed.</p>"},{"location":"autoscaling/autoscaling/#what-triggers-a-scale-up","title":"What triggers a scale up","text":"<p>The scale up is triggered if there are pods in the cluster, which are unschedulable and</p> <ul> <li>could be scheduled, if any of the node pools with autoscaling enabled would accommodate them if they would grow in size</li> <li>the node pools, which could accommodate them, are not yet at maximum size</li> </ul> <p>However, if pods' resource requests are larger than any new node would offer, the scale up will not be triggered. The cluster is scanned every 10 seconds for these pods, to assure quick response to the cluster needs. For more information, please have a look at official Cluster Autoscaler documentation.</p>"},{"location":"autoscaling/autoscaling/#what-triggers-a-scale-down","title":"What triggers a scale down","text":"<p>The scale down is triggered, if all following conditions are met</p> <ul> <li>the sum of CPU and memory requests of all pods running on node considered for scale down is below 50% (Claudie by default excludes DaemonSet pods and Mirror pods)</li> <li>all pods running on the node (except those that run on all nodes by default, like manifest-run pods or pods created by DaemonSets) considered for scale down,  can be scheduled to other nodes</li> <li>the node considered for scale down does not have scale-down disabled annotation</li> </ul> <p>For more information, please have a look at official Cluster Autoscaler documentation.</p>"},{"location":"autoscaling/autoscaling/#architecture","title":"Architecture","text":"<p>As stated earlier, Claudie deploys Cluster Autoscaler and Autoscaler Adapter for every Claudie-made cluster which enables it. These components are deployed within the same cluster as Claudie.</p> <p></p>"},{"location":"autoscaling/autoscaling/#considerations","title":"Considerations","text":"<p>As Claudie just extends Cluster Autoscaler, it is important that you follow their best practices. Furthermore, as number of nodes in autoscaled node pools can be volatile, you should carefully plan out how you will use the storage on such node pools. Longhorn support of Cluster Autoscaler is still in experimental phase (longhorn documentation).</p>"},{"location":"claudie-workflow/claudie-workflow/","title":"Claudie","text":""},{"location":"claudie-workflow/claudie-workflow/#a-single-platform-for-multiple-clouds","title":"A single platform for multiple clouds","text":""},{"location":"claudie-workflow/claudie-workflow/#microservices","title":"Microservices","text":"<ul> <li>Context-box</li> <li>Scheduler</li> <li>Builder</li> <li>Terraformer</li> <li>Ansibler</li> <li>Kube-eleven</li> <li>Kuber</li> <li>Frontend</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#data-stores","title":"Data stores","text":"<ul> <li>MongoDB</li> <li>Minio</li> <li>DynamoDB</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#tools-used","title":"Tools used","text":"<ul> <li>Terraform</li> <li>Ansible</li> <li>KubeOne</li> <li>Longhorn</li> <li>Nginx</li> <li>Calico</li> <li>gRPC</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#context-box","title":"Context-box","text":"<p>Context box is Claudie's \"control unit\". It holds pending configs, which need to be processed, periodically checks for new/changed configs and receives new configs from <code>frontend</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api","title":"API","text":"<pre><code>  // SaveConfigFrontEnd saves the config parsed by Frontend.\nrpc SaveConfigFrontEnd(SaveConfigRequest) returns (SaveConfigResponse);\n// SaveConfigScheduler saves the config parsed by Scheduler.\nrpc SaveConfigScheduler(SaveConfigRequest) returns (SaveConfigResponse);\n// SaveConfigBuilder saves the config parsed by Builder.\nrpc SaveConfigBuilder(SaveConfigRequest) returns (SaveConfigResponse);\n// GetConfigFromDB gets a single config from the database.\nrpc GetConfigFromDB(GetConfigFromDBRequest) returns (GetConfigFromDBResponse);\n// GetConfigScheduler gets a config from Scheduler's queue of pending configs.\nrpc GetConfigScheduler(GetConfigRequest) returns (GetConfigResponse);\n// GetConfigBuilder gets a config from Builder's queue of pending configs.\nrpc GetConfigBuilder(GetConfigRequest) returns (GetConfigResponse);\n// GetAllConfigs gets all configs from the database.\nrpc GetAllConfigs(GetAllConfigsRequest) returns (GetAllConfigsResponse);\n// DeleteConfig sets the manifest to null, effectively forcing the deletion of the infrastructure\n// defined by the manifest on the very next config (diff-) check.\nrpc DeleteConfig(DeleteConfigRequest) returns (DeleteConfigResponse);\n// DeleteConfigFromDB deletes the config from the database.\nrpc DeleteConfigFromDB(DeleteConfigRequest) returns (DeleteConfigResponse);\n// UpdateNodepool updates specific nodepool from the config. Used mainly for autoscaling.\nrpc UpdateNodepool(UpdateNodepoolRequest) returns (UpdateNodepoolResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Frontend, calculates its <code>msChecksum</code> and saves it to the database</li> <li>Periodically checks for <code>config</code> changes and pushes the <code>config</code> to the <code>schedulerQueue</code> if <code>msChecksum</code> != <code>dsChecksum</code></li> <li>Periodically checks for <code>config</code> changes and pushes the <code>config</code> to the <code>builderQueue</code> if <code>dsChecksum</code> != <code>csChecksum</code></li> <li>Receives a <code>config</code> with the <code>desiredState</code> from Scheduler and saves it to the database</li> <li>Receives a <code>config</code> with the <code>currentState</code> from Builder and saves it to the database</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#variables-used","title":"Variables used","text":"variable meaning <code>msChecksum</code> manifest checksum <code>dsChecksum</code> desired state checksum <code>csChecksum</code> current state checksum"},{"location":"claudie-workflow/claudie-workflow/#scheduler","title":"Scheduler","text":"<p>Scheduler brings the infrastructure to a desired the state based on the manifest contained in the config that is received from Context-box.</p> <p>Scheduler also monitors the health of current infrastructure and manages any operations based on actual health state (e.g. replacement of broken nodes, etc. [work in progress]).</p>"},{"location":"claudie-workflow/claudie-workflow/#api_1","title":"API","text":"<p>This service is a gRPC client, thus it does not provide any API</p>"},{"location":"claudie-workflow/claudie-workflow/#flow_1","title":"Flow","text":"<ul> <li>Periodically pulls <code>config</code> from Context-Box's <code>schedulerQueue</code></li> <li>Creates <code>desiredState</code> with <code>dsChecksum</code> based on the <code>config</code></li> <li>Sends the <code>config</code> file back to Context-box</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#builder","title":"Builder","text":"<p>Builder aligns the current state of the infrastructure with the desired state. It calls methods on <code>terraformer</code>, <code>ansibler</code>, <code>kube-eleven</code> and <code>kuber</code> in order to manage the infrastructure. It follows that Builder also takes care of deleting nodes from a kubernetes cluster by finding differences between <code>desiredState</code> and <code>currentState</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_2","title":"API","text":"<p>This service is a gRPC client, thus it does not provide any API</p>"},{"location":"claudie-workflow/claudie-workflow/#flow_2","title":"Flow","text":"<ul> <li>Periodically polls Context-Box's <code>builderQueue</code> for changes in <code>config</code>, pulls it when changed</li> <li>Calls Terraformer, Ansibler, Kube-eleven and Kuber</li> <li>Creates <code>currentState</code></li> <li>Sends updated <code>config</code> with the <code>currentState</code> to Context-box</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#terraformer","title":"Terraformer","text":"<p>Terraformer creates or destroys infrastructure (specified in the desired state) via Terraform calls.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_3","title":"API","text":"<pre><code>  // BuildInfrastructure builds the infrastructure based on the provided desired state (includes addition/deletion of *stuff*).\nrpc BuildInfrastructure(BuildInfrastructureRequest) returns (BuildInfrastructureResponse);\n// DestroyInfrastructure destroys the infrastructure completely.\nrpc DestroyInfrastructure(DestroyInfrastructureRequest) returns (DestroyInfrastructureResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_3","title":"Flow","text":"<ul> <li>Receives a <code>config</code> from Builder</li> <li>Uses Terraform to create infrastructure based on the <code>desiredState</code></li> <li>Updates the <code>currentState</code> in the <code>config</code></li> <li>Upon receiving a deletion request, Terraformer destroys the infrastructure based on the current state</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#ansibler","title":"Ansibler","text":"<p>Ansibler uses Ansible to:</p> <ul> <li>set up Wireguard VPN between the nodes</li> <li>set up nginx load balancer</li> <li>install dependencies for nodes in a kubernetes cluster</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#api_4","title":"API","text":"<pre><code>  // InstallNodeRequirements installs any requirements there are on all of the nodes.\nrpc InstallNodeRequirements(InstallRequest) returns (InstallResponse);\n// InstallVPN sets up a VPN between the nodes in the k8s cluster and LB clusters.\nrpc InstallVPN(InstallRequest) returns (InstallResponse);\n// SetUpLoadbalancers sets up the load balancers together with the DNS and verifies their configuration.\nrpc SetUpLoadbalancers(SetUpLBRequest) returns (SetUpLBResponse);\n// TeardownLoadBalancers correctly destroys the load balancers attached to a k8s\n// cluster by choosing a new ApiServer endpoint.\nrpc TeardownLoadBalancers(TeardownLBRequest) returns (TeardownLBResponse);\n// UpdateAPIEndpoint handles changes of API endpoint between control nodes.\n// It will update the current stage based on the information from the desired state.\nrpc UpdateAPIEndpoint(UpdateAPIEndpointRequest) returns (UpdateAPIEndpointResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_4","title":"Flow","text":"<ul> <li>Receives a <code>configToDelete</code> from Builder for <code>TeardownLoadBalancers()</code></li> <li>Finds the new ApiEndpoint among the control nodes of the k8s-cluster.</li> <li>Sets up new certs for the endpoint to be reachable</li> <li>Receives a <code>config</code> from Builder for <code>InstallVPN()</code></li> <li>Sets up ansible inventory, and installs the Wireguard full mesh VPN using a playbook</li> <li>Updates the <code>currentState</code> in a <code>config</code></li> <li>Receives a <code>config</code> from Builder for <code>InstallNodeRequirements()</code></li> <li>Sets up ansible inventory, and installs any prerequisites, as per individual nodes' requirements</li> <li>Updates the <code>currentState</code> in a <code>config</code></li> <li>Receives a <code>config</code> from Builder for <code>SetUpLoadbalancers()</code></li> <li>Sets up the ansible inventory, and installs nginx load balancers</li> <li> <p>Creates and verifies the DNS configuration for the load balancers</p> </li> <li> <p><code>UpdateAPIEndpoint()</code> is called in specific use cases when there is change  in the api endpoint of a control plane.</p> </li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#kube-eleven","title":"Kube-eleven","text":"<p>Kube-eleven uses KubeOne to set up kubernetes clusters. After cluster creation, it assures the cluster stays healthy and keeps running smoothly.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_5","title":"API","text":"<pre><code>  // BuildCluster builds the kubernetes clusters specified in the provided config.\nrpc BuildCluster(BuildClusterRequest) returns (BuildClusterResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_5","title":"Flow","text":"<ul> <li>Receives a <code>config</code> object from Builder</li> <li>Generates KubeOne manifest based on the <code>desiredState</code></li> <li>Uses KubeOne to provision a kubernetes cluster</li> <li>Updates the <code>currentState</code> in the <code>config</code></li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#kuber","title":"Kuber","text":"<p>Kuber manipulates the cluster resources using <code>kubectl</code>.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_6","title":"API","text":"<pre><code>  // RemoveLbScrapeConfig removes scrape config for every LB detached from this cluster.\nrpc RemoveLbScrapeConfig(RemoveLbScrapeConfigRequest) returns (RemoveLbScrapeConfigResponse);\n// StoreLbScrapeConfig stores scrape config for every LB attached to this cluster.\nrpc StoreLbScrapeConfig(StoreLbScrapeConfigRequest) returns (StoreLbScrapeConfigResponse);\n// StoreClusterMetadata creates a secret, which holds the private key and a list of public IP addresses of the cluster supplied.\nrpc StoreClusterMetadata(StoreClusterMetadataRequest) returns (StoreClusterMetadataResponse);\n// DeleteClusterMetadata deletes the secret holding the private key and public IP addresses of the cluster supplied.\nrpc DeleteClusterMetadata(DeleteClusterMetadataRequest) returns (DeleteClusterMetadataResponse);\n// SetUpStorage installs Longhorn into the cluster.\nrpc SetUpStorage(SetUpStorageRequest) returns (SetUpStorageResponse); // StoreKubeconfig creates a secret, which holds the kubeconfig of a Claudie-created cluster.\nrpc StoreKubeconfig(StoreKubeconfigRequest) returns (StoreKubeconfigResponse);\n// DeleteKubeconfig removes the secret that holds the kubeconfig of a Claudie-created cluster.\nrpc DeleteKubeconfig(DeleteKubeconfigRequest) returns (DeleteKubeconfigResponse);\n// DeleteNodes deletes the specified nodes from a k8s cluster.\nrpc DeleteNodes(DeleteNodesRequest) returns (DeleteNodesResponse);\n// PatchNodes uses kubectl patch to change the node manifest.\nrpc PatchNodes(PatchNodeTemplateRequest) returns (PatchNodeTemplateResponse);\n// SetUpClusterAutoscaler deploys Cluster Autoscaler and Autoscaler Adapter for every cluster specified.\nrpc SetUpClusterAutoscaler(SetUpClusterAutoscalerRequest) returns (SetUpClusterAutoscalerResponse);\n// DestroyClusterAutoscaler deletes Cluster Autoscaler and Autoscaler Adapter for every cluster specified.\nrpc DestroyClusterAutoscaler(DestroyClusterAutoscalerRequest) returns (DestroyClusterAutoscalerResponse);\n// PatchClusterInfoConfigMap updates the cluster-info config map in the kube-public namespace with the new\n// kubeconfig. This needs to be done after an api endpoint change as the config map in the kube-public namespace\n// is used by kubeadm when joining.\nrpc PatchClusterInfoConfigMap(PatchClusterInfoConfigMapRequest) returns (PatchClusterInfoConfigMapResponse);\n</code></pre>"},{"location":"claudie-workflow/claudie-workflow/#flow_6","title":"Flow","text":"<ul> <li>Recieves a <code>config</code> from Builder for <code>PatchClusterInfoConfigMap</code></li> <li>updatedes kubeconfig to reflect the new changed endpoint.</li> <li>Receives a <code>config</code> from Builder for <code>SetUpStorage()</code></li> <li>Applies the <code>longhorn</code> deployment</li> <li>Receives a <code>config</code> from Builder for <code>StoreKubeconfig()</code></li> <li>Creates a kubernetes secret that holds the kubeconfig of the Claudie-created cluster</li> <li>Receives a <code>config</code> from Builder for <code>StoreMetadata()</code></li> <li>Creates a kubernetes secret that holds the node metadata of the Claudie-created cluster</li> <li>Receives a <code>config</code> from Builder for <code>StoreLbScrapeConfig()</code></li> <li>Stores scrape config for any LB attached to the Claudie-made cluster.</li> <li>Receives a <code>config</code> from Builder for <code>PatchNodes()</code></li> <li>Patches the node manifests of the Claudie-made cluster.</li> <li>Upon infrastructure deletion request, Kuber deletes the kubeconfig secret, metadata secret, scrape configs and autoscaler of the cluster being deleted</li> </ul>"},{"location":"claudie-workflow/claudie-workflow/#frontend","title":"Frontend","text":"<p>Frontend is a layer between the user and Claudie. It is a <code>InputManifest</code> Custom Resource Definition controller, that will communicate with <code>context-box</code> to maintain the input manifest state. New manifests are added as CRD into the kubernetes cluster where Frontend pulls them and saves them to Claudie.</p>"},{"location":"claudie-workflow/claudie-workflow/#api_7","title":"API","text":"<p>This service is a gRPC client, thus it does not provide any API</p>"},{"location":"claudie-workflow/claudie-workflow/#flow_7","title":"Flow","text":"<ul> <li>User applies a new InputManifest crd holding a manifest</li> <li>Frontend detects it and processes the created/modified input manifest</li> <li>Upon deletion of user-created InputManifest, Frontend initiates a deletion process of the manifest</li> </ul>"},{"location":"contributing/contributing/","title":"Contributing","text":""},{"location":"contributing/contributing/#bug-reports","title":"Bug reports","text":"<p>When you encounter a bug, please create a new issue and use our bug template. Before you submit, please check:</p> <ul> <li>...that the issue you want to open is not a duplicate</li> <li>...that you submitted the logs/screenshots of any errors and a concise way to reproduce the issue</li> <li>...the input manifest you used </li> </ul> <p>be careful not to include your cloud credentials</p>"},{"location":"contributing/release/","title":"How to release a new version of Claudie","text":"<p>The release process of Claudie consists of a few manual steps and a few automated steps.</p>"},{"location":"contributing/release/#manual-steps","title":"Manual steps","text":"<p>Whoever is responsible for creating a new release has to:</p> <ol> <li>Write a new entry to a relevant Changelog document</li> <li>Add release notes to the Releases page</li> <li>Publish a release</li> </ol>"},{"location":"contributing/release/#automated-steps","title":"Automated steps","text":"<p>After a new release is published, a release pipeline runs, which will:</p> <ol> <li>Build new images tagged with the release tag</li> <li>Push them to the container registry where anyone can pull them</li> <li>Add Claudie manifest files to the release assets, with image tags referencing this release</li> </ol>"},{"location":"docs-guides/deployment-workflow/","title":"Documentation deployment","text":"<p>Our documentation is hosted on GitHub Pages. Whenever a new push to gh-pages branch happens, it will deploy a new version of the doc. All the commits and pushes to this branch are automated through our release-docs.yml pipeline with the usage of mike tool.</p> <p>That's also the reason, why we do not recommend making any manual changes in gh-pages branch. However, in case you have to, use the commands below.</p>"},{"location":"docs-guides/deployment-workflow/#generate-a-new-version-of-the-docs","title":"Generate a new version of the docs","text":"<ul> <li>To create new version of docs</li> </ul> <pre><code>mike deploy &lt;version&gt;\n</code></pre> <ul> <li>To deploy new version to live page</li> </ul> <pre><code>mike deploy &lt;version&gt; --push\n</code></pre> <ul> <li>To make new version the default version when visiting the docs page</li> </ul> <pre><code>mike set-default &lt;version&gt;\n</code></pre>"},{"location":"docs-guides/deployment-workflow/#deploy-docs-from-some-older-github-tags","title":"Deploy docs from some older GitHub tags","text":"<ul> <li>Checkout to the desired tag</li> </ul> <pre><code>git checkout tags/&lt;tag&gt;\n</code></pre> <ul> <li>Create new version of <code>mkdocs.yml</code></li> </ul> <p>To find out how, follow the mkdocs documentation</p> <ul> <li>Create python virtual environment</li> </ul> <pre><code>python3 -m venv ./venv\n</code></pre> <ul> <li>Activate python virtual environment</li> </ul> <pre><code>source ./venv/bin/activate\n</code></pre> <ul> <li>Install python requirements</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Deploy new version of docs</li> </ul> <pre><code>mike deploy &lt;version&gt; --push\n</code></pre>"},{"location":"docs-guides/deployment-workflow/#deploy-docs-for-a-new-release-manually","title":"Deploy docs for a new release manually","text":"<p>In case the release-docs.yml fails, you can deploy the new version manually by following this steps:</p> <ul> <li>Checkout to a new branch</li> </ul> <pre><code>git checkout tags/&lt;release tag&gt;\n</code></pre> <ul> <li>Create python virtual environment</li> </ul> <pre><code>python3 -m venv ./venv\n</code></pre> <ul> <li>Activate python virtual environment</li> </ul> <pre><code>source ./venv/bin/activate\n</code></pre> <ul> <li>Install python requirements</li> </ul> <pre><code>pip install -r requirements.txt\n</code></pre> <ul> <li>Deploy new version of docs</li> </ul> <pre><code>mike deploy &lt;release tag&gt; latest --push -u\n</code></pre> <p> Don't forget to use the <code>latest</code> tag in the last command, because otherwise the new version will not be loaded as default one, when visiting docs.claudie.io </p> <p>Find more about how to work with mike.</p>"},{"location":"docs-guides/development/","title":"Development of the Claudie official docs","text":"<p>First of all, it is worth to mention, that we are using MkDocs to generate HTML documents from markdown ones. To make our documentation prettier, we have used Material theme for MkDocs. Regarding the version of our docs we are using mike.</p>"},{"location":"docs-guides/development/#how-to-run","title":"How to run","text":"<p>First install the dependencies from requirements.txt in your local machine. However before doing that we recommend creating a virtual environment by running the command below.</p> <pre><code>python3 -m venv ./venv\n</code></pre> <p>After that you want to activate that newly create virtual environment by running:</p> <pre><code>source ./venv/bin/activate\n</code></pre> <p>Now, we can install the docs dependencies, which we mentioned before.</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>After successfull instalation, you can run command below, which generates HTML files for the docs and host in on your local server.</p> <pre><code>mkdocs serve\n</code></pre>"},{"location":"docs-guides/development/#how-to-test-changes","title":"How to test changes","text":"<p>Whenever you make some changes in docs folder or in mkdocs.yml file, you can see if the changes were applied as you expected by running the command below, which starts the server with newly generated docs.</p> <pre><code>mkdocs serve\n</code></pre> <p> Using this command you will not see the docs versioning, because we are using mike tool for this. </p> <p>In case you want to test the docs versioning, you will have to run:</p> <pre><code>mike serve\n</code></pre> <p>Keep in mind, that mike takes the docs versions from gh-pages branch. That means, you will not be able to see your changes, in case you didn't run the command below before.</p> <pre><code>mike deploy &lt;version&gt;\n</code></pre> <p> Be careful, because this command creates a new version of the docs in your local gh-pages branch. </p>"},{"location":"faq/FAQ/","title":"Frequently Asked Question","text":"<p>We have prepared some of our most frequently asked question to help you out!</p>"},{"location":"faq/FAQ/#does-claudie-make-sense-as-a-pure-k8s-orchestration-on-a-single-cloud-provider-iaas","title":"Does Claudie make sense as a pure K8s orchestration on a single cloud-provider IaaS?","text":"<p>Since Claudie specializes in multicloud, you will likely face some drawbacks, such as the need for a public IPv4 address for each node. Otherwise it works well in a single-provider mode. Using Claudie will also give you some advantages, such as scaling to multi-cloud as your needs change, or the autoscaler that Claudie provides.</p>"},{"location":"faq/FAQ/#which-scenarios-make-sense-for-using-claudie-and-which-dont","title":"Which scenarios make sense for using Claudie and which don't?","text":"<p>Claudie aims to address the following scenarios, described in more detail on the use-cases page:</p> <ul> <li>Cost savings</li> <li>Data locality</li> <li>Compliance (e.g. GDPR)</li> <li>Managed Kubernetes for cloud providers that do not offer it</li> <li>Cloud bursting</li> <li>Service interconnect</li> </ul> <p>Using Claudie doesn't make sense when you rely on specific features of a cloud provider and necessarily tying yourself to that cloud provider.</p>"},{"location":"faq/FAQ/#is-there-any-networking-performance-impact-due-to-the-introduction-of-the-vpn-layer","title":"Is there any networking performance impact due to the introduction of the VPN layer?","text":"<p>We compared the use of the VPN layer with other solutions and concluded that the impact on performance is negligible. \u2028If you are interested in performed benchmarks, we summarized the results in our blog post.</p>"},{"location":"faq/FAQ/#what-is-the-performance-impact-of-a-geographically-distributed-control-plane-in-claudie","title":"What is the performance impact of a geographically distributed control plane in Claudie?","text":"<p>We have performed several tests and problems start to appear when the control nodes are geographically about 600 km apart. Although this is not an answer that fits all scenarios and should only be taken as a reference point.</p> <p>If you are interested in the tests we have run and a more detailed answer, you can read more in our blog post.</p>"},{"location":"faq/FAQ/#does-the-cloud-provider-traffic-egress-bill-represent-a-significant-part-on-the-overall-running-costs","title":"Does the cloud provider traffic egress bill represent a significant part on the overall running costs?","text":"<p>Costs are individual and depend on the cost of the selected cloud provider and the type of workload running on the cluster based on the user's needs. Networking expenses can exceed 50% of your provider bill, therefore we recommend making your workload geography and provider aware (e.g. using taints and affinities).</p>"},{"location":"faq/FAQ/#should-i-be-worried-about-giving-claudie-provider-credentials-including-ssh-keys","title":"Should I be worried about giving Claudie provider credentials, including ssh keys?","text":"<p>Provider credentials are created as secrets in the Management Cluster for Claudie which you then reference when creating the input manifest, that is passed to Claudie. Claudie only uses the credentials to create a connection to nodes in the case of static nodepools or to provision the required infrastructure in the case of dynamic nodepools. The credentials are as secure as your secret management allows.</p> <p>We are transparent and all of our code is open-sourced, if in doubt you can always check for yourself.</p>"},{"location":"faq/FAQ/#does-each-node-need-a-public-ip-address","title":"Does each node need a public IP address?","text":"<p>For dynamic nodepools, nodes created by Claudie in specified cloud providers, each node needs a public IP, for static nodepools no public IP is needed.</p>"},{"location":"faq/FAQ/#is-a-guicliclusterapi-providerterraform-provider-planned","title":"Is a GUI/CLI/ClusterAPI provider/Terraform provider planned?","text":"<p>A GUI is not actively considered at this point in time. Other possibilities are openly discussed in this github issue.</p>"},{"location":"faq/FAQ/#what-is-the-roadmap-for-adding-support-for-new-cloud-iaas-providers","title":"What is the roadmap for adding support for new cloud IaaS providers?","text":"<p>Adding support for a new cloud provider is an easy task. Let us know your needs.</p>"},{"location":"feedback/feedback-form/","title":"Feedback form","text":"Your message:                       Send"},{"location":"getting-started/detailed-guide/","title":"Detailed guide","text":"<p>This detailed guide for Claudie serves as a resource for providing an overview of Claudie's features, installation instructions, customization options, and its role in provisioning and managing clusters. We'll start by guiding you through the process of setting up a management cluster, where Claudie will be installed, enabling you to effortlessly monitor and control clusters across multiple hyperscalers.</p> <p>Tip!</p> <p>Claudie offers extensive customization options for your Kubernetes cluster across multiple hyperscalers. This detailed guide assumes you have AWS and Hetzner accounts. You can customize your deployment across different supported providers. If you wish to use different providers, we recommend to follow this guide anyway and create your own input manifest file based on the provided example. Refer to the supported provider table for the input manifest configuration of each provider.</p>"},{"location":"getting-started/detailed-guide/#supported-providers","title":"Supported providers","text":"Supported Provider Node Pools DNS AWS Azure GCP OCI Hetzner Cloudflare N/A <p>For adding support for other cloud providers, open an issue or propose a PR.</p>"},{"location":"getting-started/detailed-guide/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Kind by following the Kind documentation.</li> <li>Install kubectl tool to communicate with your management cluster by following the Kubernetes documentation. </li> <li>Install Kustomize by following Kustomize documentation.</li> <li>Install Docker by following Docker documentation.</li> </ol>"},{"location":"getting-started/detailed-guide/#claudie-deployment","title":"Claudie deployment","text":"<ol> <li> <p>Create a Kind cluster where you will deploy Claudie, also referred to as the Management Cluster.</p> <pre><code>kind create cluster --name=claudie\n</code></pre> <p>Management cluster consideration.</p> <p>We recommend using a non-ephemeral management cluster! Deleting the management cluster prevents autoscaling of Claudie node pools as well as loss of state! We recommended to use a managed Kubernetes offerings to ensure management cluster resiliency. Kind cluster is sufficient for this guide.</p> </li> <li> <p>Check if have the correct current kubernetes context. The context should be <code>kind-claudie</code>.</p> <pre><code>kubectl config current-context\n</code></pre> </li> <li> <p>If context is not <code>kind-claudie</code>, switch to it:</p> <pre><code>kubectl config use-context kind-claudie\n</code></pre> </li> <li> <p>One of the prerequisites is <code>cert-manager</code>, deploy it with the following command:</p> <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml\n</code></pre> </li> <li> <p>Download latest Claudie release and unzip it:</p> <pre><code>wget https://github.com/berops/claudie/releases/latest/download/claudie.zip &amp;&amp; unzip claudie.zip -d claudie\n</code></pre> <p>Tip!</p> <p>For the initial attempt, it's highly recommended to enable debug logs, especially when creating a large cluster with DNS. This helps identify and resolve any permission issues that may occur across different hyperscalers. Edit <code>claudie/.env</code> file, and change <code>GOLANG_LOG=info</code> to <code>GOLANG_LOG=debug</code> to enable debug logging, for more customization refer to this table.</p> </li> <li> <p>Deploy Claudie using Kustomize plugin:     <pre><code>kubectl apply -k claudie\n</code></pre></p> </li> <li> <p>Claudie will be deployed into <code>claudie</code> namespace, you can view if all pods are running:</p> <p><pre><code>kubectl get pods -n claudie </code></pre> <pre><code>NAME                           READY   STATUS      RESTARTS        AGE\nansibler-5c6c776b75-82c2q      1/1     Running     0               8m10s\nbuilder-59f9d44596-n2qzm       1/1     Running     0               8m10s\ncontext-box-5d76c89b4d-tb6h4   1/1     Running     1 (6m37s ago)   8m10s\ncreate-table-job-jvs9n         0/1     Completed   1               8m10s\ndynamodb-68777f9787-8wjhs      1/1     Running     0               8m10s\nfrontend-5755b7bc69-5l84h      1/1     Running     0               8m10s\nkube-eleven-64468cd5bd-qp4d4   1/1     Running     0               8m10s\nkuber-698c4564c-dhsvg          1/1     Running     0               8m10s\nmake-bucket-job-fb5sp          0/1     Completed   0               8m10s\nminio-0                        1/1     Running     0               8m10s\nminio-1                        1/1     Running     0               8m10s\nminio-2                        1/1     Running     0               8m10s\nminio-3                        1/1     Running     0               8m10s\nmongodb-67bf769957-9ct5z       1/1     Running     0               8m10s\nscheduler-654cbd4b97-qwtbf     1/1     Running     0               8m10s\nterraformer-fd664b7ff-dd2h7    1/1     Running     0               8m9s\n</code></pre></p> <p>Troubleshoot!</p> <p>If you experience problems refer to our troubleshooting guide. </p> </li> <li> <p>Let's create a AWS high availability cluster which we'll expand later on with Hetzner bursting capacity. Let's start by creating providers secrets for the infrastructure, and next we will reference them in <code>inputmanifest-bursting.yaml</code>.</p> <pre><code># AWS provider requires the secrets to have fields: accesskey and secretkey\nkubectl create secret generic aws-secret-1 --namespace=mynamespace --from-literal=accesskey='SLDUTKSHFDMSJKDIALASSD' --from-literal=secretkey='iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD'\nkubectl create secret generic aws-secret-dns --namespace=mynamespace --from-literal=accesskey='ODURNGUISNFAIPUNUGFINB' --from-literal=secretkey='asduvnva+skd/ounUIBPIUjnpiuBNuNipubnPuip'    </code></pre> <pre><code># inputmanifest-bursting.yaml\n\napiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: cloud-bursting\nspec:\nproviders:\n- name: aws-1\nproviderType: aws\nsecretRef:\nname: aws-secret-1\nnamespace: mynamespace\n- name: aws-dns\nproviderType: aws\nsecretRef:\nname: aws-secret-dns\nnamespace: mynamespace    nodePools:\ndynamic:\n- name: aws-controlplane\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1a\ncount: 3\nserverType: t3.medium\nimage: ami-0965bd5ba4d59211c\n- name: aws-worker\nproviderSpec:\nname: aws-1\nregion: eu-north-1\nzone: eu-north-1a\ncount: 3\nserverType: t3.medium\nimage: ami-03df6dea56f8aa618\nstorageDiskSize: 200\n- name: aws-loadbalancer\nproviderSpec:\nname: aws-1\nregion: eu-central-2\nzone: eu-central-2a\ncount: 2\nserverType: t3.small\nimage: ami-0965bd5ba4d59211c\nkubernetes:\nclusters:\n- name: my-super-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- aws-controlplane\ncompute:\n- aws-worker\nloadBalancers:\nroles:\n- name: apiserver\nprotocol: tcp\nport: 6443\ntargetPort: 6443\ntarget: k8sControlPlane\nclusters:\n- name: loadbalance-me\nroles:\n- apiserver\ndns:\ndnsZone: domain.com # hosted zone domain name where claudie creates dns records for this cluster\nprovider: aws-dns\nhostname: supercluster # the sub domain of the new cluster\ntargetedK8s: my-super-cluster\npools:\n- aws-loadbalancer\n</code></pre> <p>Tip!</p> <p>In this example, two AWS providers are used \u2014 one with access to compute resources and the other with access to DNS. However, it is possible to use a single AWS provider with permissions for both services.</p> </li> <li> <p>Apply the <code>InputManifest</code> crd with your cluster configuration file:</p> <pre><code>kubectl apply -f ./inputmanifest-bursting.yaml\n</code></pre> <p>Tip!</p> <p>InputManifests serve as a single source of truth for both Claudie and the user, which makes creating infrastructure via input manifests as infrastructure as a code and can be easily integrated into a GitOps workflow.</p> <p>Errors in input manifest</p> <p>Validation webhook will reject the InputManifest at this stage if it finds errors within the manifest. Refer to our API guide for details.</p> </li> <li> <p>View logs from <code>frontend</code> service to see secret picked up, as well as which service is currently doing the work:</p> <p>View the <code>InputManifest</code> state with <code>kubectl</code></p> <p><pre><code>kubectl get inputmanifests.claudie.io cloud-bursting -o jsonpath={.status} | jq .\n</code></pre> Here\u2019s an example of what frontend might output at this point:</p> <pre><code>  {\n\"clusters\": {\n\"my-super-cluster\": {\n\"message\": \" installing VPN\",\n\"phase\": \"ANSIBLER\",\n\"state\": \"IN_PROGRESS\"\n}\n},\n\"state\": \"IN_PROGRESS\"\n}\n</code></pre> <p>Claudie architecture</p> <p>Claudie utilizes multiple services for cluster provisioning, refer to our workflow documentation as to how it works under the hood. Frontend's consolidated log provides visibility into the ongoing operations of each individual Claudie service. </p> <p>Provisioning times may vary!</p> <p>Please note that cluster creation time may vary due to provisioning capacity and machine provisioning times of selected hyperscalers.</p> <p>After finishing the <code>InputManifest</code> state reflects that the cluster is provisioned.</p> <pre><code>kubectl get inputmanifests.claudie.io cloud-bursting -o jsonpath={.status} | jq .\n{\n\"clusters\": {\n\"my-super-cluster\": {\n\"phase\": \"NONE\",\n\"state\": \"DONE\"\n}\n},\n\"state\": \"DONE\"\n}    </code></pre> </li> <li> <p>Claudie creates kubeconfig secret in claudie namespace:</p> <p><pre><code>kubectl get secrets -l claudie.io/output=kubeconfig\n</code></pre> <pre><code>NAME                                  TYPE     DATA   AGE\nmy-super-cluster-6ktx6rb-kubeconfig   Opaque   1      134m\n</code></pre></p> <p>You can recover kubeconfig for your cluster with the following command:</p> <pre><code>kubectl get secrets -n claudie -l claudie.io/output=kubeconfig -o jsonpath='{.items[0].data.kubeconfig}' | base64 -d &gt; my-super-cluster-kubeconfig.yaml\n</code></pre> <p>If you want to connect to your machines via SSH, you can recover private SSH key:</p> <pre><code>kubectl get secrets -n claudie -l claudie.io/output=metadata -ojsonpath='{.items[0].data.metadata}' | base64 -d | jq -r .private_key &gt; ~/.ssh/my-super-cluster\n</code></pre> <p>To recover public IP of a node to connect to via SSH: <pre><code>kubectl get secrets -n claudie -l claudie.io/output=metadata -ojsonpath='{.items[0].data.metadata}' | base64 -d | jq -r .node_ips\n</code></pre></p> <p>Each secret created by Claudie has following labels:</p> Key Value <code>claudie.io/project</code> Name of the project. <code>claudie.io/cluster</code> Name of the cluster. <code>claudie.io/cluster-id</code> ID of the cluster. <code>claudie.io/output</code> Output type, either <code>kubeconfig</code> or <code>metadata</code>. </li> <li> <p>Use your new kubeconfig to see what\u2019s in your new cluster</p> <pre><code>kubectl get pods -A --kubeconfig=my-super-cluster-kubeconfig.yaml\n</code></pre> </li> <li> <p>Let's add a bursting autoscaling node pool in Hetzner cloud. In order to use other hyperscalers, we'll need to add a new provider with appropriate credentials. First we will create a provider secret for Hetzner Cloud, then we open <code>inputmanifest-bursting.yaml</code> input manifest again and append the new Hetzner node pool configuration.</p> <pre><code># Hetzner provider requires the secrets to have field: credentials\nkubectl create secret generic hetzner-secret-1 --namespace=mynamespace --from-literal=credentials='kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp'\n</code></pre> <p>Claudie autoscaling</p> <p>Autoscaler in Claudie is deployed in Claudie management cluster and provisions additional resources remotely at the time of need. For more information check out how Claudie autoscaling works.</p> <pre><code># inputmanifest-bursting.yaml\n\napiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: cloud-bursting\nspec:\nproviders:\n- name: hetzner-1         # add under nodePools.dynamic section\nproviderType: hetzner\nsecretRef:\nname: hetzner-secret-1\nnamespace: mynamespace        nodePools:\ndynamic:\n...\n- name: hetzner-worker  # add under nodePools.dynamic section\nproviderSpec:\nname: hetzner-1   # use your new hetzner provider hetzner-1 to create these nodes\nregion: hel1\nzone: hel1-dc2\nserverType: cpx51\nimage: ubuntu-22.04\nautoscaler:           # this node pool uses a claudie autoscaler instead of static count of nodes\nmin: 1\nmax: 10\nkubernetes:\nclusters:\n- name: my-super-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- aws-controlplane\ncompute:\n- aws-worker\n- hetzner-worker  # add it to the compute list here\n...\n</code></pre> </li> <li> <p>Update the crd with the new InputManifest to incorporate the desired changes.</p> <p>Deleting existing secrets!</p> <p>Deleting or replacing existing input manifest secrets triggers cluster deletion! To add new components to your existing clusters, generate a new secret value and apply it using the following command.</p> <pre><code>kubectl apply -f ./inputmanifest-bursting.yaml\n</code></pre> </li> <li> <p>You can also passthrough additional ports from load balancers to control plane and or worker node pools by adding additional roles under <code>roles</code>.     <pre><code># inputmanifest-bursting.yaml\n\napiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: cloud-bursting\nspec:\n...\nloadBalancers:\nroles:\n- name: apiserver\nprotocol: tcp\nport: 6443\ntargetPort: 6443\ntarget: k8sControlPlane\n- name: https\nprotocol: tcp\nport: 443\ntargetPort: 443\ntarget: k8sComputeNodes # only loadbalance between workers\nclusters:\n- name: loadbalance-me\nroles:\n- apiserver\n- https # define it here\ndns:\ndnsZone: domain.com\nprovider: aws-dns\nhostname: supercluster\ntargetedK8s: my-super-cluster\npools:\n- aws-loadbalancer\n</code></pre>     !!! note Load balancing         Please refer how our load balancing works by reading our documentation.</p> </li> <li> <p>Update the InputManifest again with the new configuration.     <pre><code>kubectl apply -f ./inputmanifest-bursting.yaml\n</code></pre></p> </li> <li> <p>To delete the cluster just simply delete the secret and wait for Claudie to destroy it.</p> <pre><code>kubectl delete -f ./inputmanifest-bursting.yaml\n</code></pre> <p>Removing clusters</p> <p>Deleting Claudie or the management cluster does not remove the Claudie managed clusters. Delete the secret first to initiate Claudie's deletion process.</p> </li> <li> <p>After frontend finished deletion workflow delete minikube cluster      <pre><code>kind delete cluster\n</code></pre></p> </li> </ol>"},{"location":"getting-started/detailed-guide/#general-tips","title":"General tips","text":""},{"location":"getting-started/detailed-guide/#control-plane-considerations","title":"Control plane considerations","text":"<ul> <li>Single Control Plane Node: Node pool with one machine manages your cluster.</li> <li>Multiple Control Plane Nodes: Control plane node pool that has more than one node.<ul> <li>Load Balancer Requirement: A load balancer is optional for high availability setup, however we recommend it. Include an additional node pool for load balancers.</li> <li>DNS Requirement: If you want to use load balancing, you will need a registered domain name, and a hosted zone. Claudie creates a failover DNS record for the load balancer machines.<ul> <li>Supported DNS providers: If your DNS provider is not supported, delegate a subdomain to a supported DNS provider, refer to supported DNS providers.</li> </ul> </li> <li>Egress Traffic: Hyperscalers charge for outbound data and multi-region infrastructure. To avoid egress traffic deploy control plane node pools in the same region to one hypoerscaler. If availability is more important than egress traffic costs, you can have multiple control plane node pools spanning across different hyperscalers.</li> </ul> </li> </ul>"},{"location":"getting-started/detailed-guide/#egress-traffic","title":"Egress traffic","text":"<p>Hyperscalers charge for outbound data and multi-region infrastructure.</p> <ul> <li> <p>Control plane: To avoid egress traffic deploy control plane node pools in the same region to one hyperscaler. If availability is more important than egress traffic costs, you can have multiple control plane node pools spanning across different hyperscalers.</p> </li> <li> <p>Workloads: Egress costs associated with workloads are more complicated as they depend on each use case. What we recommend it to try and use localised workloads where possible. </p> </li> </ul> <p>Example</p> <p>Consider a scenario where you have a workload that involves processing extensive datasets from GCP storage using Claudie managed AWS GPU instances. To minimize egress network traffic costs, it is recommended to host the datasets in an S3 bucket and limit egress traffic from GCP and keep the workload localised.</p>"},{"location":"getting-started/detailed-guide/#on-your-own-path","title":"On your own path","text":"<p>Once you've gained a comprehensive understanding of how Claudie operates through this guide, you can deploy it to a reliable management cluster, this could be a cluster that you already have. Tailor your input manifest file to suit your specific requirements and explore a detailed example showcasing providers, load balancing, and DNS records across various hyperscalers by visiting this comprehensive example.</p>"},{"location":"getting-started/detailed-guide/#claudie-customization","title":"Claudie customization","text":"<p>All of the customisable settings can be found in <code>claudie/.env</code> file.</p> Variable Default Type Description <code>GOLANG_LOG</code> <code>info</code> string Log level for all services. Can be either <code>info</code> or <code>debug</code>. <code>DATABASE_HOSTNAME</code> <code>mongodb</code> string Database hostname used for Claudie configs. <code>CONTEXT_BOX_HOSTNAME</code> <code>context-box</code> string Context-box service hostname. <code>TERRAFORMER_HOSTNAME</code> <code>terraformer</code> string Terraformer service hostname. <code>ANSIBLER_HOSTNAME</code> <code>ansibler</code> string Ansibler service hostname. <code>KUBE_ELEVEN_HOSTNAME</code> <code>kube-eleven</code> string Kube-eleven service hostname. <code>KUBER_HOSTNAME</code> <code>kuber</code> string Kuber service hostname. <code>MINIO_HOSTNAME</code> <code>minio</code> string MinIO hostname used for state files. <code>DYNAMO_HOSTNAME</code> <code>dynamo</code> string DynamoDB hostname used for lock files. <code>DYNAMO_TABLE_NAME</code> <code>claudie</code> string Table name for DynamoDB lock files. <code>AWS_REGION</code> <code>local</code> string Region for DynamoDB lock files. <code>DATABASE_PORT</code> 27017 int Port of the database service. <code>TERRAFORMER_PORT</code> 50052 int Port of the Terraformer service. <code>ANSIBLER_PORT</code> 50053 int Port of the Ansibler service. <code>KUBE_ELEVEN_PORT</code> 50054 int Port of the Kube-eleven service. <code>CONTEXT_BOX_PORT</code> 50055 int Port of the Context-box service. <code>KUBER_PORT</code> 50057 int Port of the Kuber service. <code>MINIO_PORT</code> 9000 int Port of the MinIO service. <code>DYNAMO_PORT</code> 8000 int Port of the DynamoDB service."},{"location":"getting-started/get-started-using-claudie/","title":"Getting started","text":""},{"location":"getting-started/get-started-using-claudie/#get-started-using-claudie","title":"Get started using Claudie","text":""},{"location":"getting-started/get-started-using-claudie/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, please make sure you have the following prerequisites installed and set up:</p> <ol> <li> <p>Claudie needs to be installed on an existing Kubernetes cluster, referred to as the Management Cluster, which it uses to manage the clusters it provisions. For testing, you can use ephemeral clusters like Minikube or Kind. However, for production environments, we recommend using a more resilient solution since Claudie maintains the state of the infrastructure it creates.</p> </li> <li> <p>Claudie requires the installation of cert-manager in your Management Cluster. To install cert-manager, use the following command:     <pre><code>kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml\n</code></pre></p> </li> </ol>"},{"location":"getting-started/get-started-using-claudie/#supported-providers","title":"Supported providers","text":"Supported Provider Node Pools DNS AWS Azure GCP OCI Hetzner Cloudflare N/A <p>For adding support for other cloud providers, open an issue or propose a PR.</p>"},{"location":"getting-started/get-started-using-claudie/#install-claudie","title":"Install Claudie","text":"<ol> <li> <p>Download and extract Claudie manifests from our release page:     <pre><code>wget https://github.com/berops/claudie/releases/latest/download/claudie.zip &amp;&amp; unzip claudie.zip -d claudie\n</code></pre></p> </li> <li> <p>Deploy Claudie to the Management Cluster:     <pre><code>kubectl apply -k claudie\n</code></pre></p> </li> </ol>"},{"location":"getting-started/get-started-using-claudie/#deploy-your-cluster","title":"Deploy your cluster","text":"<ol> <li> <p>Create Kubernetes Secret resource for your provider configuration.</p> <pre><code>kubectl create secret generic example-aws-secret-1 \\\n--namespace=mynamespace \\\n--from-literal=accesskey='myAwsAccessKey' \\\n--from-literal=secretkey='myAwsSecretKey'\n</code></pre> <p>Check the supported providers for input manifest examples. For an input manifest spanning all supported hyperscalers checkout out this example.</p> </li> <li> <p>Deploy InputManifest resource which Claudie uses to create infrastructure, include the created secret in <code>.spec.providers</code> as follows:     <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\n  name: examplemanifest\nspec:\n  providers:\n      - name: aws-1\n      providerType: aws\n      secretRef:\n          name: example-aws-secret-1 # reference the secret name\n          namespace: mynamespace     # reference the secret namespace\n  nodePools:\n      dynamic:\n      - name: control-aws\n          providerSpec:\n            name: aws-1\n            region: eu-central-1\n            zone: eu-central-1a\n          count: 1\n          serverType: t3.medium\n          image: ami-0965bd5ba4d59211c\n      - name: compute-1-aws\n          providerSpec:\n            name: aws-1\n            region: eu-central-2\n            zone: eu-central-2a\n          count: 2\n          serverType: t3.medium\n          image: ami-0965bd5ba4d59211c\n          storageDiskSize: 50\n  kubernetes:\n      clusters:\n      - name: aws-cluster\n          version: v1.24.0\n          network: 192.168.2.0/24\n          pools:\n            control:\n                - control-aws\n            compute:\n                - compute-1-aws        \nEOF\n</code></pre></p> <p>Deleting existing InputManifest resource deletes provisioned infrastructure!</p> </li> </ol>"},{"location":"getting-started/get-started-using-claudie/#connect-to-your-cluster","title":"Connect to your cluster","text":"<p>Claudie outputs base64 encoded kubeconfig secret <code>&lt;cluster-name&gt;-&lt;cluster-hash&gt;-kubeconfig</code> in the namespace where it is deployed:</p> <ol> <li>Recover kubeconfig of your cluster by running:     <pre><code>kubectl get secrets -n claudie -l claudie.io/output=kubeconfig -o jsonpath='{.items[0].data.kubeconfig}' | base64 -d &gt; your_kubeconfig.yaml\n</code></pre></li> <li>Use your new kubeconfig:     <pre><code>kubectl get pods -A --kubeconfig=your_kubeconfig.yaml\n</code></pre></li> </ol>"},{"location":"getting-started/get-started-using-claudie/#cleanup","title":"Cleanup","text":"<ol> <li>To remove your cluster and its associated infrastructure, delete the cluster definition block from the InputManifest:     <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\n  name: examplemanifest\nspec:\n  providers:\n      - name: aws-1\n      providerType: aws\n      secretRef:\n          name: example-aws-secret-1 # reference the secret name\n          namespace: mynamespace     # reference the secret namespace\n  nodePools:\n      dynamic:\n      - name: control-aws\n          providerSpec:\n            name: aws-1\n            region: eu-central-1\n            zone: eu-central-1a\n          count: 1\n          serverType: t3.medium\n          image: ami-0965bd5ba4d59211c\n      - name: compute-1-aws\n          providerSpec:\n            name: aws-1\n            region: eu-central-2\n            zone: eu-central-2a\n          count: 2\n          serverType: t3.medium\n          image: ami-0965bd5ba4d59211c\n          storageDiskSize: 50\n  kubernetes:\n    clusters:\n#      - name: aws-cluster\n#          version: v1.24.0\n#          network: 192.168.2.0/24\n#          pools:\n#            control:\n#                - control-aws\n#            compute:\n#                - compute-1-aws         \nEOF\n</code></pre></li> <li> <p>To delete all clusters defined in the input manifest, delete the InputManifest. This triggers the deletion process, removing the infrastructure and all data associated with the manifest.</p> <pre><code>kubectl delete inputmanifest examplemanifest\n</code></pre> </li> </ol>"},{"location":"input-manifest/api-reference/","title":"InputManifest API reference","text":"<p>InputManifest is a definition of the user's infrastructure. It contains cloud provider specification, nodepool specification, Kubernetes and loadbalancer clusters.</p>"},{"location":"input-manifest/api-reference/#status","title":"Status","text":"<p>Most recently observed status of the InputManifest</p>"},{"location":"input-manifest/api-reference/#spec","title":"Spec","text":"<p>Specification of the desired behavior of the InputManifest</p> <ul> <li><code>providers</code> Providers</li> </ul> <p>Providers is a list of defined cloud provider configuration that will be used in infrastructure provisioning.</p> <ul> <li><code>nodepools</code> Nodepools</li> </ul> <p>Describes nodepools used for either kubernetes clusters or loadbalancer cluster defined in this manifest.</p> <ul> <li><code>kubernetes</code> Kubernetes</li> </ul> <p>List of Kubernetes cluster this manifest will manage.</p> <ul> <li><code>loadBalancers</code> Loadbalancer</li> </ul> <p>List of loadbalancer clusters the Kubernetes clusters may use.</p>"},{"location":"input-manifest/api-reference/#providers","title":"Providers","text":"<p>Contains configurations for supported cloud providers. At least one provider needs to be defined.</p> <ul> <li><code>name</code></li> </ul> <p>The name of the provider specification. It has to be unique across all providers.</p> <ul> <li><code>providerType</code></li> </ul> <p>Type of a provider. The providerType defines mandatory fields that has to be included for a specific provider. A list of available providers can be found at providers section. Allowed values are:</p> Value Description <code>aws</code> AWS provider type <code>azure</code> Azure provider type <code>cloudflare</code> Cloudflare provider type <code>gcp</code> GCP provider type <code>hetzner</code> Hetzner provider type <code>hetznerdns</code> Hetzner DNS provider type <code>oci</code> OCI provider type <ul> <li><code>secretRef</code> SecretRef</li> </ul> <p>Represents a Secret Reference. It has enough information to retrieve secret in any namespace.</p> <p>Support for more cloud providers is in the roadmap.</p>"},{"location":"input-manifest/api-reference/#secretref","title":"SecretRef","text":"<p>SecretReference represents a Kubernetes Secret Reference. It has enough information to retrieve secret in any namespace.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the secret, which holds data for the particular cloud provider instance.</p> <ul> <li><code>namespace</code></li> </ul> <p>Namespace of the secret which holds data for the particular cloud provider instance.</p>"},{"location":"input-manifest/api-reference/#cloudflare","title":"Cloudflare","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the Cloudflare provider. To find out how to configure Cloudflare follow the instructions here</p> <ul> <li><code>apitoken</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/api-reference/#hetznerdns","title":"HetznerDNS","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the HetznerDNS provider. To find out how to configure HetznerDNS follow the instructions here</p> <ul> <li><code>apitoken</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/api-reference/#gcp","title":"GCP","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the GCP provider. To find out how to configure GCP provider and service account, follow the instructions here.</p> <ul> <li><code>credentials</code></li> </ul> <p>Credentials for the provider. Stringified JSON service account key.</p> <ul> <li><code>gcpproject</code></li> </ul> <p>Project id of an already existing GCP project where the infrastructure is to be created.</p>"},{"location":"input-manifest/api-reference/#hetzner","title":"Hetzner","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the Hetzner provider. To find out how to configure Hetzner provider and service account, follow the instructions here.</p> <ul> <li><code>credentials</code></li> </ul> <p>Credentials for the provider (API token).</p>"},{"location":"input-manifest/api-reference/#oci","title":"OCI","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the OCI provider. To find out how to configure OCI provider and service account, follow the instructions here.</p> <ul> <li><code>privatekey</code></li> </ul> <p>Private key used to authenticate to the OCI.</p> <ul> <li><code>keyfingerprint</code></li> </ul> <p>Fingerprint of the user-supplied private key.</p> <ul> <li><code>tenancyocid</code></li> </ul> <p>OCID of the tenancy where <code>privateKey</code> is added as an API key</p> <ul> <li><code>userocid</code></li> </ul> <p>OCID of the user in the supplied tenancy</p> <ul> <li><code>compartmentocid</code></li> </ul> <p>OCID of the compartment where VMs/VCNs/... will be created</p>"},{"location":"input-manifest/api-reference/#aws","title":"AWS","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the AWS provider. To find out how to configure AWS provider and service account, follow the instructions here.</p> <ul> <li><code>accesskey</code></li> </ul> <p>Access key ID for your AWS account.</p> <ul> <li><code>secretkey</code></li> </ul> <p>Secret key for the Access key specified above.</p>"},{"location":"input-manifest/api-reference/#azure","title":"Azure","text":"<p>The fields that need to be included in a Kubernetes Secret resource to utilize the Azure provider. To find out how to configure Azure provider and service account, follow the instructions here.</p> <ul> <li><code>subscriptionid</code></li> </ul> <p>Subscription ID of your subscription in Azure.</p> <ul> <li><code>tenantid</code></li> </ul> <p>Tenant ID of your tenancy in Azure.</p> <ul> <li><code>clientid</code></li> </ul> <p>Client ID of your client. The Claudie is design to use a service principal with appropriate permissions.</p> <ul> <li><code>clientsecret</code></li> </ul> <p>Client secret generated for your client.</p>"},{"location":"input-manifest/api-reference/#nodepools","title":"Nodepools","text":"<p>Collection of static and dynamic nodepool specification, to be referenced in the <code>kubernetes</code> or <code>loadBalancer</code> clusters.</p> <ul> <li><code>dynamic</code> Dynamic</li> </ul> <p>List of dynamically to-be-created nodepools of not yet existing machines, used for Kubernetes or loadbalancer clusters.</p> <p>These are only blueprints, and will only be created per reference in <code>kubernetes</code> or <code>loadBalancer</code> clusters. E.g. if the nodepool isn't used, it won't even be created. Or if the same nodepool is used in two different clusters, it will be created twice. In OOP analogy, a dynamic nodepool would be a class that would get instantiated <code>N &gt;= 0</code> times depending on which clusters reference it.</p> <ul> <li><code>static</code> [WORK IN PROGRESS]</li> </ul> <p>List of static nodepools of already existing machines, not created by of Claudie, used for Kubernetes or loadbalancer clusters. Typically, these would be on-premises machines.</p>"},{"location":"input-manifest/api-reference/#dynamic","title":"Dynamic","text":"<p>Dynamic nodepools are defined for cloud provider machines that Claudie is expected to create.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the nodepool. Each nodepool will have a random hash appended to the name, so the whole name will be of format <code>&lt;name&gt;-&lt;hash&gt;</code>.</p> <ul> <li><code>provideSpec</code> Provider spec</li> </ul> <p>Collection of provider data to be used while creating the nodepool.  </p> <ul> <li><code>count</code></li> </ul> <p>Number of the nodes in the nodepool. Mutually exclusive with <code>autoscaler</code>.</p> <ul> <li><code>serverType</code></li> </ul> <p>Type of the machines in the nodepool.</p> <p>Currently, only AMD64 machines are supported.</p> <ul> <li><code>image</code></li> </ul> <p>OS image of the machine.</p> <p>Currently, only Ubuntu 22.04 AMD64 images are supported.</p> <ul> <li><code>storageDiskSize</code></li> </ul> <p>Size of the storage disk on the nodes in the nodepool in <code>GB</code>. The OS disk is created automatically with predefined size of <code>100GB</code> for kubernetes nodes and <code>50GB</code> for Loadbalancer nodes.</p> <p>Default value is <code>50</code>, minimum value is <code>50</code>. Value is used only for compute nodes.</p> <p>This field is optional, however, if compute nodepool does not define it, default value is used for creation of storage disk. Control nodepools and Loadbalancer nodepools ignore this field.</p> <ul> <li><code>autoscaler</code> Autoscaler Configuration</li> </ul> <p>Autoscaler configuration for this nodepool. Mutually exclusive with <code>count</code>.</p>"},{"location":"input-manifest/api-reference/#provider-spec","title":"Provider Spec","text":"<p>Provider spec is an additional specification built on top of the data from any of the provider instance. Here are provider configuration examples for each individual provider: aws, azure, gcp, cloudflare, hetzner and oci.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the provider instance specified in providers</p> <ul> <li><code>region</code></li> </ul> <p>Region of the nodepool.</p> <ul> <li><code>zone</code></li> </ul> <p>Zone of the nodepool.</p>"},{"location":"input-manifest/api-reference/#autoscaler-configuration","title":"Autoscaler Configuration","text":"<p>Autoscaler configuration on per nodepool basis. Defines the number of nodes, autoscaler will scale up or down specific nodepool.</p> <ul> <li><code>min</code></li> </ul> <p>Minimum number of nodes in nodepool.</p> <ul> <li><code>max</code></li> </ul> <p>Maximum number of nodes in nodepool.</p>"},{"location":"input-manifest/api-reference/#kubernetes","title":"Kubernetes","text":"<p>Defines Kubernetes clusters.</p> <ul> <li><code>clusters</code> Cluster-k8s</li> </ul> <p>List of Kubernetes clusters Claudie will create.</p>"},{"location":"input-manifest/api-reference/#cluster-k8s","title":"Cluster-k8s","text":"<p>Collection of data used to define a Kubernetes cluster.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the Kubernetes cluster. Each cluster will have a random hash appended to the name, so the whole name will be of format <code>&lt;name&gt;-&lt;hash&gt;</code>.</p> <ul> <li><code>version</code></li> </ul> <p>Kubernetes version of the cluster.</p> <p>Version should be defined in format <code>vX.Y</code>. In terms of supported versions of Kubernetes, Claudie follows <code>kubeone</code> releases and their supported versions. The current <code>kubeone</code> version used in Claudie is <code>1.5</code>. To see the list of supported versions, please refer to <code>kubeone</code> documentation.</p> <ul> <li><code>network</code></li> </ul> <p>Network range for the VPN of the cluster. The value should be defined in format <code>A.B.C.D/mask</code>.</p> <ul> <li><code>pools</code></li> </ul> <p>List of nodepool names this cluster will use. Remember that nodepools defined in nodepools are only \"blueprints\". The actual nodepool will be created once referenced here.</p>"},{"location":"input-manifest/api-reference/#loadbalancer","title":"LoadBalancer","text":"<p>Defines loadbalancer clusters.</p> <ul> <li><code>roles</code> Role</li> </ul> <p>List of roles loadbalancers use to forward the traffic. Single role can be used in multiple loadbalancer clusters.</p> <ul> <li><code>clusters</code> Cluster-lb</li> </ul> <p>List of loadbalancer clusters used in the Kubernetes clusters defined under clusters.</p>"},{"location":"input-manifest/api-reference/#role","title":"Role","text":"<p>Role defines a concrete loadbalancer configuration. Single loadbalancer can have multiple roles.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the role. Used as a reference in clusters.</p> <ul> <li><code>protocol</code></li> </ul> <p>Protocol of the rule. Allowed values are:</p> Value Description <code>tcp</code> Role will use TCP protocol <code>udp</code> Role will use UDP protocol <ul> <li><code>port</code></li> </ul> <p>Port of the incoming traffic on the loadbalancer.</p> <ul> <li><code>targetPort</code></li> </ul> <p>Port where loadbalancer forwards the traffic.</p> <ul> <li><code>target</code></li> </ul> <p>Defines a target group of nodes. Allowed values are:</p> Value Description <code>k8sAllNodes</code> All nodes in the cluster <code>k8sControlNodes</code> Only control/master nodes in cluster <code>k8sComputeNodes</code> Only compute/worker nodes in cluster"},{"location":"input-manifest/api-reference/#cluster-lb","title":"Cluster-lb","text":"<p>Collection of data used to define a loadbalancer cluster.</p> <ul> <li><code>name</code></li> </ul> <p>Name of the loadbalancer.</p> <ul> <li><code>roles</code></li> </ul> <p>List of roles the loadbalancer uses.</p> <ul> <li><code>dns</code> DNS</li> </ul> <p>Specification of the loadbalancer's DNS record.</p> <ul> <li><code>targetedK8s</code></li> </ul> <p>Name of the Kubernetes cluster targetted by this loadbalancer.</p> <ul> <li><code>pools</code></li> </ul> <p>List of nodepool names this loadbalancer will use. Remember, that nodepools defined in nodepools are only \"blueprints\". The actual nodepool will be created once referenced here.</p>"},{"location":"input-manifest/api-reference/#dns","title":"DNS","text":"<p>Collection of data Claudie uses to create a DNS record for the loadbalancer.</p> <ul> <li><code>dnsZone</code></li> </ul> <p>DNS zone inside of which the records will be created. GCP/AWS/OCI/Azure/Cloudflare/Hetzner DNS zone is accepted</p> <ul> <li><code>provider</code></li> </ul> <p>Name of provider to be used for creating an A record entry in defined DNS zone.</p> <ul> <li><code>hostname</code></li> </ul> <p>Custom hostname for your A record. If left empty, the hostname will be a random hash.</p>"},{"location":"input-manifest/example/","title":"Example yaml file","text":"example.yaml<pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: ExampleManifest\nspec:\n# Providers field is used for defining the providers. \n# It is referncing a secret resource in Kubernetes cluster.\n# Each provider haves its own mandatory fields that are defined in the secret resource.\n# Every supported provider has an example in this input manifest.\n# providers:\n#   - name: \n#       providerType:   # type of the provider secret [aws|azure|gcp|oci|hetzner|hetznerdns|cloudflare]    \n#       secretRef:      # secret reference specyfication\n#         name:         # name of the secret resource\n#         namespace:    # namespace of the secret resoutce\nproviders:\n# Hetzner DNS provider.\n- name: hetznerdns-1\nproviderType: hetznerdns\nsecretRef:\nname: hetznerdns-secret-1\nnamespace: example-namespace\n\n# Cloudflare DNS provider.\n- name: cloudflare-1\nproviderType: cloudflare\nsecretRef:\nname: cloudflare-secret-1\nnamespace: example-namespace\n\n# Hetzner Cloud provider.\n- name: hetzner-1\nproviderType: hetzner\nsecretRef:\nname: hetzner-secret-1\nnamespace: example-namespace\n\n# GCP cloud provider.\n- name: gcp-1\nproviderType: gcp\nsecretRef:\nname: gcp-secret-1\nnamespace: example-namespace\n\n# OCI cloud provider.\n- name: oci-1\nproviderType: oci\nsecretRef:\nname: oci-secret-1\nnamespace: example-namespace\n\n# AWS cloud provider.\n- name: aws-1\nproviderType: aws\nsecretRef:\nname: aws-secret-1\nnamespace: example-namespace\n\n# Azure cloud provider.\n- name: azure-1\nproviderType: azure\nsecretRef:\nname: azure-secret-1\nnamespace: example-namespace\n\n\n# Nodepools field is used for defining the nodepool specification.\n# You can think of them as a blueprints, not actual nodepools that will be created.\nnodePools:\n# Dynamic nodepools are created by Claudie, in one of the cloud providers specified.\n# Definition specification:\n# dynamic:\n#   - name:             # Name of the nodepool, which is used as a refference to it. Needs to be unique.\n#     providerSpec:     # Provider specification for this nodepool.\n#       name:           # Name of the provider instance, referencing one of the providers define above.\n#       region:         # Region of the nodepool.\n#       zone:           # Zone of the nodepool.\n#     count:            # Static number of nodes in this nodepool.\n#     serverType:       # Machine type of the nodes in this nodepool.\n#     image:            # OS image of the nodes in the nodepool.\n#     storageDiskSize:  # Disk size of the storage disk for compute nodepool.\n#     autoscaler:       # Autoscaler configuration. Mutually exclusive with Count.\n#       min:            # Minimum number of nodes in nodepool.\n#       max:            # Maximum number of nodes in nodepool.\n#\n# Example definitions for each provider\ndynamic:\n- name: control-hetzner\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 3\nserverType: cpx11\nimage: ubuntu-22.04\n\n- name: compute-hetzner\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 2\nserverType: cpx11\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\n- name: compute-hetzner-autoscaled\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\nserverType: cpx11\nimage: ubuntu-22.04\nstorageDiskSize: 50\nautoscaler:\nmin: 1\nmax: 5\n\n- name: control-gcp\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 3\nserverType: e2-medium\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: compute-gcp\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 2\nserverType: e2-small\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\n- name: control-oci\nproviderSpec:\nname: oci-1\nregion: eu-milan-1\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 3\nserverType: VM.Standard2.1\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: compute-oci\nproviderSpec:\nname: oci-1\nregion: eu-milan-1\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 2\nserverType: VM.Standard2.1\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\n- name: control-aws\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserverType: t3.medium\nimage: ami-0965bd5ba4d59211c\n\n- name: compute-aws\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserverType: t3.medium\nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\n- name: control-azure\nproviderSpec:\nname: azure-1\nregion: West Europe\nzone: \"1\"\ncount: 2\nserverType: Standard_B2s\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: compute-azure\nproviderSpec:\nname: azure-1\nregion: West Europe\nzone: \"1\"\ncount: 2\nserverType: Standard_B2s\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\n- name: loadbalancer-1\nprovider:\nproviderSpec:\nname: gcp-1\nregion: europe-west1\nzone: europe-west1-c\ncount: 2\nserverType: e2-small\nimage: ubuntu-os-cloud/ubuntu-2004-focal-v20220610\n\n- name: loadbalancer-2\nproviderSpec:\nname: hetzner-1\nregion: hel1\nzone: hel1-dc2\ncount: 2\nserverType: cpx11\nimage: ubuntu-20.04\n\n# Kubernetes field is used to define the kubernetes clusters.\n# Definition specification:\n#\n# clusters:\n#   - name:           # Name of the cluster. The name will be appended to the created node name.\n#     version:        # Kubernetes version in semver scheme, must be supported by KubeOne.\n#     network:        # Private network IP range.\n#     pools:          # Nodepool names which cluster will be composed of. User can reuse same nodepool specification on multiple clusters.\n#       control:      # List of nodepool names, which will be used as control nodes.\n#       compute:      # List of nodepool names, which will be used as compute nodes.\n#\n# Example definitions:\nkubernetes:\nclusters:\n- name: dev-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner\n- control-gcp\ncompute:\n- compute-hetzner\n- compute-gcp\n- compute-azure\n\n- name: prod-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner\n- control-gcp\n- control-oci\n- control-aws\n- control-azure\ncompute:\n- compute-hetzner\n- compute-gcp\n- compute-oci\n- compute-aws\n- compute-azure\n\n# Loadbalancers field defines loadbalancers used for the kubernetes clusters and roles for the loadbalancers.\n# Definition specification for role:\n#\n# roles:\n#   - name:         # Name of the role, used as a reference later. Must be unique.\n#     protocol:     # Protocol, this role will use.\n#     port:         # Port, where trafic will be coming.\n#     targetPort:   # Port, where loadbalancer will forward traffic to.\n#     target:       # Targeted nodes on kubernetes cluster. Can be \"k8sControlPlane\", \"k8sComputePlane\" or \"k8sAllNodes\".\n#\n# Definition specification for loadbalancer:\n#\n# clusters:\n#   - name:         # Loadbalancer cluster name\n#     roles:        # List of role names this loadbalancer will fullfil.\n#     dns:          # DNS specification, where DNS records will be created.\n#       dnsZone:    # DNS zone name in your provider.\n#       provider:   # Provider name for the DNS.\n#       hostname:   # Hostname for the DNS record. Keep in mind the zone will be included automaticaly. If left empty the Claudie will create random hash as a hostname.\n#     targetedK8s:  # Name of the targeted kubernetes cluster\n#     pools:        # List of nodepool names used for loadbalancer\n# Example definitons:\nloadBalancers:\nroles:\n- name: apiserver\nprotocol: tcp\nport: 6443\ntargetPort: 6443\ntarget: k8sControlPlane\n\nclusters:\n- name: apiserver-lb-dev\nroles:\n- apiserver\ndns:\ndnsZone: dns-zone\nprovider: hetznerdns-1\ntargetedK8s: dev-cluster\npools:\n- loadbalancer-1\n- name: apiserver-lb-prod\nroles:\n- apiserver\ndns:\ndnsZone: dns-zone\nprovider: cloudflare-1\nhostname: my.fancy.url\ntargetedK8s: prod-cluster\npools:\n- loadbalancer-2\n</code></pre>"},{"location":"input-manifest/providers/aws/","title":"AWS","text":"<p>AWS cloud provider requires you to input the credentials as an <code>accesskey</code> and a <code>secretkey</code>.</p>"},{"location":"input-manifest/providers/aws/#compute-and-dns-example","title":"Compute and DNS example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: aws-secret\ndata:\naccesskey: U0xEVVRLU0hGRE1TSktESUFMQVNTRA==\nsecretkey: aXVoYk9JSk4rb2luL29saWtEU2Fkc25vaVNWU0RzYWNvaW5PVVNIRA==\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/aws/#create-aws-credentials","title":"Create AWS credentials","text":""},{"location":"input-manifest/providers/aws/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install AWS CLI tools by following this guide.</li> <li>Setup AWS CLI on your machine by following this guide.</li> </ol>"},{"location":"input-manifest/providers/aws/#creating-aws-credentials-for-claudie","title":"Creating AWS credentials for Claudie","text":"<ol> <li> <p>Create a user using AWS CLI:     <pre><code>aws iam create-user --user-name claudie\n</code></pre></p> </li> <li> <p>Create a policy document with compute and DNS permissions required by Claudie:     <pre><code>cat &gt; policy.json &lt;&lt;EOF\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"ec2:*\"\n         ],\n         \"Resource\":\"*\"\n      },\n      {\n         \"Effect\":\"Allow\",\n         \"Action\":[\n            \"route53:*\"\n         ],\n         \"Resource\":\"*\"\n      }\n   ]\n}\nEOF\n</code></pre></p> <p>DNS permissions</p> <p>Exclude route53 permissions from the policy document, if you prefer not to use AWS as the DNS provider.</p> </li> <li> <p>Attach the policy to the claudie user:     <pre><code>aws iam put-user-policy --user-name claudie --policy-name ec2-and-dns-access --policy-document file://policy.json\n</code></pre></p> </li> <li> <p>Create access keys for claudie user:     <pre><code>aws iam create-access-key --user-name claudie\n</code></pre> <pre><code>{\n\"AccessKey\":{\n\"UserName\":\"claudie\",\n\"AccessKeyId\":\"AKIAIOSFODNN7EXAMPLE\",\n\"Status\":\"Active\",\n\"SecretAccessKey\":\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\",\n\"CreateDate\":\"2018-12-14T17:34:16Z\"\n}\n}\n</code></pre></p> </li> </ol>"},{"location":"input-manifest/providers/aws/#dns-setup","title":"DNS setup","text":"<p>If you wish to use AWS as your DNS provider where Claudie creates DNS records pointing to Claudie managed clusters, you will need to create a public hosted zone by following this guide. </p> <p>AWS is not my domain registrar</p> <p>If you haven't acquired a domain via AWS and wish to utilize AWS for hosting your zone, you can refer to this guide on AWS nameservers. However, if you prefer not to use the entire domain, an alternative option is to delegate a subdomain to AWS.</p>"},{"location":"input-manifest/providers/aws/#input-manifest-examples","title":"Input manifest examples","text":""},{"location":"input-manifest/providers/aws/#create-a-secret-for-aws-provider","title":"Create a secret for AWS provider","text":"<p>The secret for an AWS provider must include the following mandatory fields: <code>accesskey</code> and <code>secretkey</code>.</p> <pre><code>kubectl create secret generic aws-secret-1 --namespace=mynamespace --from-literal=accesskey='SLDUTKSHFDMSJKDIALASSD' --from-literal=secretkey='iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD'\n</code></pre>"},{"location":"input-manifest/providers/aws/#single-provider-multi-region-cluster-example","title":"Single provider, multi region cluster example","text":"<pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: AWSExampleManifest\nspec:\n\nproviders:\n- name: aws-1\nproviderType: aws\nsecretRef:\nname: aws-secret-1\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-aws\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-1\n# Availability zone of the nodepool.\nzone: eu-central-1a\ncount: 1\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\n\n- name: compute-1-aws\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-2\n# Availability zone of the nodepool.\nzone: eu-central-2a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\n- name: compute-2-aws\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-3\n# Availability zone of the nodepool.\nzone: eu-central-3a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: aws-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-aws\ncompute:\n- compute-1-aws\n- compute-2-aws\n</code></pre>"},{"location":"input-manifest/providers/aws/#multi-provider-multi-region-clusters-example","title":"Multi provider, multi region clusters example","text":"<pre><code>kubectl create secret generic aws-secret-1 --namespace=mynamespace --from-literal=accesskey='SLDUTKSHFDMSJKDIALASSD' --from-literal=secretkey='iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD'\nkubectl create secret generic aws-secret-2 --namespace=mynamespace --from-literal=accesskey='ODURNGUISNFAIPUNUGFINB' --from-literal=secretkey='asduvnva+skd/ounUIBPIUjnpiuBNuNipubnPuip'\n</code></pre> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: AWSExampleManifest\nspec:\n\nproviders:\n- name: aws-1\nproviderType: aws\nsecretRef:\nname: aws-secret-1\nnamespace: mynamespace\n- name: aws-2\nproviderType: aws\nsecretRef:\nname: aws-secret-2\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-aws-1\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\nregion: eu-central-1\n# Availability zone of the nodepool.\nzone: eu-central-1a\ncount: 1\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\n\n- name: control-aws-2\nproviderSpec:\n# Name of the provider instance.\nname: aws-2\n# Region of the nodepool.\nregion: eu-north-1\n# Availability zone of the nodepool.\nzone: eu-north-1a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-03df6dea56f8aa618\n\n- name: compute-aws-1\nproviderSpec:\n# Name of the provider instance.\nname: aws-1\n# Region of the nodepool.\nregion: eu-central-2\n# Availability zone of the nodepool.\nzone: eu-central-2a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-0965bd5ba4d59211c\nstorageDiskSize: 50\n\n- name: compute-aws-2\nproviderSpec:\n# Name of the provider instance.\nname: aws-2\n# Region of the nodepool.\nregion: eu-north-3\n# Availability zone of the nodepool.\nzone: eu-north-3a\ncount: 2\n# Instance type name.\nserverType: t3.medium\n# AMI ID of the image.\n# Make sure to update it according to the region. \nimage: ami-03df6dea56f8aa618\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: aws-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-aws-1\n- control-aws-2\ncompute:\n- compute-aws-1\n- compute-aws-2\n</code></pre>"},{"location":"input-manifest/providers/azure/","title":"Azure","text":"<p>Azure provider requires you to input <code>clientsecret</code>, <code>subscriptionid</code>, <code>tenantid</code>, and <code>clientid</code>.</p>"},{"location":"input-manifest/providers/azure/#compute-and-dns-example","title":"Compute and DNS example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: azure-secret\ndata:\nclientid: QWJjZH5FRmd+SDZJamtsc35BQkMxNXNFRkdLNTRzNzhYfk9sazk=\n# all resources you define will be charged here\nclientsecret: NmE0ZGZzZzctc2Q0di1mNGFkLWRzdmEtYWQ0djYxNmZkNTEy\nsubscriptionid: NTRjZGFmYTUtc2R2cy00NWRzLTU0NnMtZGY2NTFzZmR0NjE0\ntenantid: MDI1NXNjMjMtNzZ3ZS04N2c2LTk2NGYtYWJjMWRlZjJnaDNs\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/azure/#create-azure-credentials","title":"Create Azure credentials","text":""},{"location":"input-manifest/providers/azure/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Azure CLI by following this guide.</li> <li>Login to Azure this guide.</li> </ol>"},{"location":"input-manifest/providers/azure/#creating-azure-credentials-for-claudie","title":"Creating Azure credentials for Claudie","text":"<ol> <li> <p>Login to Azure with the following command:     <pre><code>az login\n</code></pre></p> </li> <li> <p>Permissions file for the new role that claudie service principal will use:     <pre><code>cat &gt; policy.json &lt;&lt;EOF\n{\n   \"Name\":\"Resource Group Management\",\n   \"Id\":\"bbcd72a7-2285-48ef-bn72-f606fba81fe7\",\n   \"IsCustom\":true,\n   \"Description\":\"Create and delete Resource Groups.\",\n   \"Actions\":[\n      \"Microsoft.Resources/subscriptions/resourceGroups/write\",\n      \"Microsoft.Resources/subscriptions/resourceGroups/delete\"\n   ],\n   \"AssignableScopes\":[\"/\"]\n}\nEOF\n</code></pre></p> </li> <li> <p>Create a role based on the policy document:     <pre><code>az role definition create --role-definition policy.json\n</code></pre></p> </li> <li> <p>Create a service account to access virtual machine resources as well as DNS:     <pre><code>az ad sp create-for-rbac --name claudie-sp\n</code></pre> <pre><code>{\n\"clientId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\"displayName\": \"claudie-sp\",\n\"clientSecret\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n\"tenant\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n}\n</code></pre></p> </li> <li> <p>Assign required roles for the service principal:     <pre><code>{\naz role assignment create --assignee claudie-sp --role \"Virtual Machine Contributor\"\naz role assignment create --assignee claudie-sp --role \"Network Contributor\"\naz role assignment create --assignee claudie-sp --role \"Resource Group Management\"\n}\n</code></pre></p> </li> </ol>"},{"location":"input-manifest/providers/azure/#dns-requirements","title":"DNS requirements","text":"<p>If you wish to use Azure as your DNS provider where Claudie creates DNS records pointing to Claudie managed clusters, you will need to create a public DNS zone by following this guide.</p> <p>Azure is not my domain registrar</p> <p>If you haven't acquired a domain via Azure and wish to utilize Azure for hosting your zone, you can refer to this guide on Azure nameservers. However, if you prefer not to use the entire domain, an alternative option is to delegate a subdomain to Azure.</p>"},{"location":"input-manifest/providers/azure/#input-manifest-examples","title":"Input manifest examples","text":""},{"location":"input-manifest/providers/azure/#single-provider-multi-region-cluster-example","title":"Single provider, multi region cluster example","text":""},{"location":"input-manifest/providers/azure/#create-a-secret-for-azure-provider","title":"Create a secret for Azure provider","text":"<p>The secret for an Azure provider must include the following mandatory fields: <code>clientsecret</code>, <code>subscriptionid</code>, <code>tenantid</code>, and <code>clientid</code>.</p> <pre><code>kubectl create secret generic azure-secret-1 --namespace=mynamespace --from-literal=clientsecret='Abcd~EFg~H6Ijkls~ABC15sEFGK54s78X~Olk9' --from-literal=subscriptionid='6a4dfsg7-sd4v-f4ad-dsva-ad4v616fd512' --from-literal=tenantid='54cdafa5-sdvs-45ds-546s-df651sfdt614' --from-literal=clientid='0255sc23-76we-87g6-964f-abc1def2gh3l'\n</code></pre> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: AzureExampleManifest\nspec:\nproviders:\n- name: azure-1\nproviderType: azure\nsecretRef:\nname: azure-secret-1\nnamespace: mynamespace\nnodePools:\ndynamic:\n- name: control-azure\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: \"1\"\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: compute-1-azure\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: Germany West Central\n# Zone of the nodepool.\nzone: \"1\"\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\n- name: compute-2-azure\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: \"1\"\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: azure-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-azure\ncompute:\n- compute-2-azure\n- compute-1-azure\n</code></pre>"},{"location":"input-manifest/providers/azure/#multi-provider-multi-region-clusters-example","title":"Multi provider, multi region clusters example","text":"<pre><code>kubectl create secret generic azure-secret-1 --namespace=mynamespace --from-literal=clientsecret='Abcd~EFg~H6Ijkls~ABC15sEFGK54s78X~Olk9' --from-literal=subscriptionid='6a4dfsg7-sd4v-f4ad-dsva-ad4v616fd512' --from-literal=tenantid='54cdafa5-sdvs-45ds-546s-df651sfdt614' --from-literal=clientid='0255sc23-76we-87g6-964f-abc1def2gh3l'\n\nkubectl create secret generic azure-secret-2 --namespace=mynamespace --from-literal=clientsecret='Efgh~ijkL~on43noi~NiuscviBUIds78X~UkL7' --from-literal=subscriptionid='0965bd5b-usa3-as3c-ads1-csdaba6fd512' --from-literal=tenantid='55safa5d-dsfg-546s-45ds-d51251sfdaba' --from-literal=clientid='076wsc23-sdv2-09cA-8sd9-oigv23npn1p2'\n</code></pre> <pre><code>name: AzureExampleManifest\napiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: AzureExampleManifest\nspec:\nproviders:\n- name: azure-1\nproviderType: azure\nsecretRef:\nname: azure-secret-1\nnamespace: mynamespace\n\n- name: azure-2\nproviderType: azure\nsecretRef:\nname: azure-secret-2\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-azure-1\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: \"1\"\ncount: 1\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: control-azure-2\nproviderSpec:\n# Name of the provider instance.\nname: azure-2\n# Location of the nodepool.\nregion: Germany West Central\n# Zone of the nodepool.\nzone: \"2\"\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\n\n- name: compute-azure-1\nproviderSpec:\n# Name of the provider instance.\nname: azure-1\n# Location of the nodepool.\nregion: Germany West Central\n# Zone of the nodepool.\nzone: \"2\"\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\n- name: compute-azure-2\nproviderSpec:\n# Name of the provider instance.\nname: azure-2\n# Location of the nodepool.\nregion: West Europe\n# Zone of the nodepool.\nzone: \"3\"\ncount: 2\n# VM size name.\nserverType: Standard_B2s\n# URN of the image.\nimage: Canonical:0001-com-ubuntu-minimal-jammy:minimal-22_04-lts:22.04.202212120\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: azure-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-azure-1\n- control-azure-2\ncompute:\n- compute-azure-1\n- compute-azure-2\n</code></pre>"},{"location":"input-manifest/providers/cloudflare/","title":"Cloudflare","text":"<p>Cloudflare provider requires <code>apitoken</code> token field in string format.</p>"},{"location":"input-manifest/providers/cloudflare/#dns-example","title":"DNS example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: cloudflare-secret\ndata:\napitoken: a3NsSVNBODc4YTZldFlBZlhZY2c1aVl5ckZHTmxDeGM=\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/cloudflare/#create-cloudflare-credentials","title":"Create Cloudflare credentials","text":"<p>You can create Cloudflare API token by following this guide. The required permissions for the zone you want to use are:</p> <pre><code>Zone:Read\nDNS:Read\nDNS:Edit\n</code></pre>"},{"location":"input-manifest/providers/cloudflare/#dns-setup","title":"DNS setup","text":"<p>If you wish to use Cloudflare as your DNS provider where Claudie creates DNS records pointing to Claudie managed clusters, you will need to create a public DNS zone by following this guide.</p> <p>Cloudflare is not my domain registrar</p> <p>If you haven't acquired a domain via Cloudflare and wish to utilize Cloudflare for hosting your zone, you can refer to this guide on Cloudflare nameservers. However, if you prefer not to use the entire domain, an alternative option is to delegate a subdomain to Cloudflare.</p>"},{"location":"input-manifest/providers/cloudflare/#input-manifest-examples","title":"Input manifest examples","text":""},{"location":"input-manifest/providers/cloudflare/#load-balancing-example","title":"Load balancing example","text":"<p>Showcase example</p> <p>To make this example functional, you need to specify control plane and node pools. This current showcase will produce an error if used as is.</p>"},{"location":"input-manifest/providers/cloudflare/#create-a-secret-for-cloudflare-and-aws-providers","title":"Create a secret for Cloudflare and AWS providers","text":"<p>The secret for an Cloudflare provider must include the following mandatory fields: <code>apitoken</code>. <pre><code>kubectl create secret generic cloudflare-secret-1 --namespace=mynamespace --from-literal=apitoken='kslISA878a6etYAfXYcg5iYyrFGNlCxc'\n</code></pre></p> <p>The secret for an AWS provider must include the following mandatory fields: <code>accesskey</code> and <code>secretkey</code>. <pre><code>kubectl create secret generic aws-secret-1 --namespace=mynamespace --from-literal=accesskey='SLDUTKSHFDMSJKDIALASSD' --from-literal=secretkey='iuhbOIJN+oin/olikDSadsnoiSVSDsacoinOUSHD'\n</code></pre></p> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: CloudflareExampleManifest\nspec:\nproviders:\n- name: cloudflare-1\nproviderType: cloudflare\nsecretRef:\nname: cloudflare-secret-1\nnamespace: mynamespace\n\n- name: aws-1\nproviderType: aws\nsecretRef:\nname: aws-secret-1\nnamespace: mynamespace\n\nnodePools: dynamic:\n- name: loadbalancer\nproviderSpec:\nname: aws-1\nregion: eu-central-1\nzone: eu-central-1c\ncount: 2\nserverType: t3.medium\nimage: ami-0965bd5ba4d59211c\n\nkubernetes:\nclusters:\n- name: cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol: []\ncompute: []\n\nloadBalancers:\nroles:\n- name: apiserver\nprotocol: tcp\nport: 6443\ntargetPort: 6443\ntarget: k8sControlPlane\n\nclusters:\n- name: apiserver-lb-prod\nroles:\n- apiserver\ndns:\ndnsZone: dns-zone\nprovider: cloudflare-1\nhostname: my.fancy.url\ntargetedK8s: prod-cluster\npools:\n- loadbalancer-2\n</code></pre>"},{"location":"input-manifest/providers/gcp/","title":"GCP","text":"<p>GCP provider requires you to input multiline <code>credentials</code> as well as specific GCP project <code>gcpproject</code> where to provision resources.</p>"},{"location":"input-manifest/providers/gcp/#compute-and-dns-example","title":"Compute and DNS example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: gcp-secret\ndata:\ncredentials: &gt;-\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\ngcpproject: cHJvamVjdC1jbGF1ZGll\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/gcp/#create-gcp-credentials","title":"Create GCP credentials","text":""},{"location":"input-manifest/providers/gcp/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install gcoud CLI on your machine by following this guide.</li> <li>Initialize gcloud CLI by following this guide.</li> <li>Authorize cloud CLI by following this guide</li> </ol>"},{"location":"input-manifest/providers/gcp/#creating-gcp-credentials-for-claudie","title":"Creating GCP credentials for Claudie","text":"<ol> <li> <p>Create a GCP project: <pre><code>gcloud projects create claudie-project\n</code></pre></p> </li> <li> <p>Set the current project to claudie-project: <pre><code>gcloud config set project claudie-project\n</code></pre></p> </li> <li> <p>Attach billing account to your project: <pre><code>gcloud alpha billing accounts projects link claudie-project (--account-id=ACCOUNT_ID | --billing-account=ACCOUNT_ID)\n</code></pre></p> </li> <li> <p>Enable Compute Engine API and Cloud DNS API: <pre><code>{\ngcloud services enable compute.googleapis.com\n  gcloud services enable dns.googleapis.com\n}\n</code></pre></p> </li> <li> <p>Create a service account: <pre><code>gcloud iam service-accounts create claudie-sa\n</code></pre></p> </li> <li> <p>Attach roles to the servcie account: <pre><code>{\ngcloud projects add-iam-policy-binding claudie-project --member=serviceAccount:claudie-sa@claudie-project.iam.gserviceaccount.com --role=roles/compute.admin\n  gcloud projects add-iam-policy-binding claudie-project --member=serviceAccount:claudie-sa@claudie-project.iam.gserviceaccount.com --role=roles/dns.admin\n}\n</code></pre></p> </li> <li> <p>Recover service account keys for claudie-sa: <pre><code>gcloud iam service-accounts keys create claudie-credentials.json --iam-account=claudie-sa@claudie-project.iam.gserviceaccount.com\n</code></pre></p> </li> </ol>"},{"location":"input-manifest/providers/gcp/#dns-setup","title":"DNS setup","text":"<p>If you wish to use GCP as your DNS provider where Claudie creates DNS records pointing to Claudie managed clusters, you will need to create a public DNS zone by following this guide.</p> <p>GCP is not my domain registrar</p> <p>If you haven't acquired a domain via GCP and wish to utilize GCP for hosting your zone, you can refer to this guide on GCP nameservers. However, if you prefer not to use the entire domain, an alternative option is to delegate a subdomain to GCP.</p>"},{"location":"input-manifest/providers/gcp/#input-manifest-examples","title":"Input manifest examples","text":""},{"location":"input-manifest/providers/gcp/#single-provider-multi-region-cluster-example","title":"Single provider, multi region cluster example","text":""},{"location":"input-manifest/providers/gcp/#create-a-secret-for-cloudflare-and-gcp-providers","title":"Create a secret for Cloudflare and GCP providers","text":"<p>The secret for an GCP provider must include the following mandatory fields: <code>gcpproject</code> and <code>credentials</code>. <pre><code># The ./claudie-credentials.json file is the file created in #Creating GCP credentials for Claudie step 7.\nkubectl create secret generic gcp-secret-1 --namespace=mynamespace --from-literal=gcpproject='project-claudie' --from-file=credentials=./claudie-credentials.json\n</code></pre></p> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: GCPExampleManifest\nspec:\nproviders:\n- name: gcp-1\nproviderType: gcp\nsecretRef:\nname: gcp-secret-1\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-gcp\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-c\ncount: 1\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: compute-1-gcp\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west3\n# Zone of the nodepool.\nzone: europe-west3-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\n- name: compute-2-gcp\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west2\n# Zone of the nodepool.\nzone: europe-west2-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: gcp-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-gcp\ncompute:\n- compute-1-gcp\n- compute-2-gcp\n</code></pre>"},{"location":"input-manifest/providers/gcp/#multi-provider-multi-region-clusters-example","title":"Multi provider, multi region clusters example","text":""},{"location":"input-manifest/providers/gcp/#create-a-secret-for-cloudflare-and-gcp-providers_1","title":"Create a secret for Cloudflare and GCP providers","text":"<p>The secret for an GCP provider must include the following mandatory fields: <code>gcpproject</code> and <code>credentials</code>. <pre><code># The ./claudie-credentials.json file is the file created in #Creating GCP credentials for Claudie step 7.\nkubectl create secret generic gcp-secret-1 --namespace=mynamespace --from-literal=gcpproject='project-claudie' --from-file=credentials=./claudie-credentials.json\nkubectl create secret generic gcp-secret-2 --namespace=mynamespace --from-literal=gcpproject='project-claudie' --from-file=credentials=./claudie-credentials-2.json\n</code></pre></p> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: GCPExampleManifest\nspec:\nproviders:\n- name: gcp-1\nproviderType: gcp\nsecretRef:\nname: gcp-secret-1\nnamespace: mynamespace\n- name: gcp-2\nproviderType: gcp\nsecretRef:\nname: gcp-secret-2\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-gcp-1\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-c\ncount: 1\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: control-gcp-2\nproviderSpec:\n# Name of the provider instance.\nname: gcp-2\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\n\n- name: compute-gcp-1\nproviderSpec:\n# Name of the provider instance.\nname: gcp-1\n# Region of the nodepool.\nregion: europe-west3\n# Zone of the nodepool.\nzone: europe-west3-a\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\n- name: compute-gcp-2\nproviderSpec:\n# Name of the provider instance.\nname: gcp-2\n# Region of the nodepool.\nregion: europe-west1\n# Zone of the nodepool.\nzone: europe-west1-c\ncount: 2\n# Machine type name.\nserverType: e2-medium\n# OS image name.\nimage: ubuntu-os-cloud/ubuntu-2204-jammy-v20221206\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: gcp-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-gcp-1\n- control-gcp-2\ncompute:\n- compute-gcp-1\n- compute-gcp-2\n</code></pre>"},{"location":"input-manifest/providers/hetzner/","title":"Hetzner","text":"<p>Hetzner provider requires <code>credentials</code> token field in string format, and Hetzner DNS provider requires <code>apitoken</code> field in string format.</p>"},{"location":"input-manifest/providers/hetzner/#compute-example","title":"Compute example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: hetzner-secret\ndata:\ncredentials: a3NsSVNBODc4YTZldFlBZlhZY2c1aVl5ckZHTmxDeGNJQ28wNjBIVkV5Z2pGczIxbnNrZTc2a3NqS2tvMjFscA==\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/hetzner/#dns-example","title":"DNS example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: hetznerdns-secret\ndata:\napitoken: a1V0UmcxcGdqQ1JhYXBQbWQ3cEFJalZnaHVyWG8xY24=\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/hetzner/#create-hetzner-api-credentials","title":"Create Hetzner API credentials","text":"<p>You can create Hetzner API credentials by following this guide. The required permissions for the zone you want to use are:</p> <pre><code>Read &amp; Write\n</code></pre>"},{"location":"input-manifest/providers/hetzner/#create-hetzner-dns-credentials","title":"Create Hetzner DNS credentials","text":"<p>You can create Hetzner DNS credentials by following this guide.</p> <p>DNS provider specification</p> <p>The provider for DNS is different from the one for the Cloud.</p>"},{"location":"input-manifest/providers/hetzner/#dns-setup","title":"DNS setup","text":"<p>If you wish to use Hetzner as your DNS provider where Claudie creates DNS records pointing to Claudie managed clusters, you will need to create a public DNS zone by following this guide.</p> <p>Hetzner is not my domain registrar</p> <p>If you haven't acquired a domain via Hetzner and wish to utilize Hetzner for hosting your zone, you can refer to this guide on Hetzner nameservers. However, if you prefer not to use the entire domain, an alternative option is to delegate a subdomain to Hetzner.</p>"},{"location":"input-manifest/providers/hetzner/#input-manifest-examples","title":"Input manifest examples","text":""},{"location":"input-manifest/providers/hetzner/#single-provider-multi-region-cluster-example","title":"Single provider, multi region cluster example","text":""},{"location":"input-manifest/providers/hetzner/#create-a-secret-for-hetzner-provider","title":"Create a secret for Hetzner provider","text":"<p>The secret for an Hetzner provider must include the following mandatory fields: <code>credentials</code>.</p> <pre><code>kubectl create secret generic hetzner-secret-1 --namespace=mynamespace --from-literal=credentials='kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp'\n</code></pre> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: HetznerExampleManifest\nspec:\nproviders:\n- name: hetzner-1\nproviderType: hetzner\nsecretRef:\nname: hetzner-secret-1\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-hetzner\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: hel1\n# Datacenter of the nodepool.\nzone: hel1-dc2\ncount: 1\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\n\n- name: compute-1-hetzner\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: fsn1\n# Datacenter of the nodepool.\nzone: fsn1-dc14\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\n- name: compute-2-hetzner\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: nbg1\n# Datacenter of the nodepool.\nzone: nbg1-dc3\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: hetzner-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner\ncompute:\n- compute-1-hetzner\n- compute-2-hetzner\n</code></pre>"},{"location":"input-manifest/providers/hetzner/#multi-provider-multi-region-clusters-example","title":"Multi provider, multi region clusters example","text":""},{"location":"input-manifest/providers/hetzner/#create-a-secret-for-hetzner-provider_1","title":"Create a secret for Hetzner provider","text":"<p>The secret for an Hetzner provider must include the following mandatory fields: <code>credentials</code>.</p> <pre><code>kubectl create secret generic hetzner-secret-1 --namespace=mynamespace --from-literal=credentials='kslISA878a6etYAfXYcg5iYyrFGNlCxcICo060HVEygjFs21nske76ksjKko21lp'\nkubectl create secret generic hetzner-secret-2 --namespace=mynamespace --from-literal=credentials='kslIIOUYBiuui7iGBYIUiuybpiUB87bgPyuCo060HVEygjFs21nske76ksjKko21l'\n</code></pre> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: HetznerExampleManifest\nspec:\nproviders:\n- name: hetzner-1\nproviderType: hetzner\nsecretRef:\nname: hetzner-secret-1\nnamespace: mynamespace\n- name: hetzner-2\nproviderType: hetzner\nsecretRef:\nname: hetzner-secret-2\nnamespace: mynamespace        nodePools:\ndynamic:\n- name: control-hetzner-1\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: hel1\n# Datacenter of the nodepool.\nzone: hel1-dc2\ncount: 1\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\n\n- name: control-hetzner-2\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-2\n# Region of the nodepool.\nregion: fsn1\n# Datacenter of the nodepool.\nzone: fsn1-dc14\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\n\n- name: compute-hetzner-1\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-1\n# Region of the nodepool.\nregion: fsn1\n# Datacenter of the nodepool.\nzone: fsn1-dc14\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\n- name: compute-hetzner-2\nproviderSpec:\n# Name of the provider instance.\nname: hetzner-2\n# Region of the nodepool.\nregion: nbg1\n# Datacenter of the nodepool.\nzone: nbg1-dc3\ncount: 2\n# Machine type name.\nserverType: cpx11\n# OS image name.\nimage: ubuntu-22.04\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: hetzner-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-hetzner-1\n- control-hetzner-2\ncompute:\n- compute-hetzner-1\n- compute-hetzner-2\n</code></pre>"},{"location":"input-manifest/providers/oci/","title":"OCI","text":"<p>OCI provider requires you to input <code>privatekey</code>, <code>keyfingerprint</code>, <code>tenancyocid</code>, <code>userocid</code>, and <code>compartmentocid</code>.</p>"},{"location":"input-manifest/providers/oci/#compute-and-dns-example","title":"Compute and DNS example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: oci-secret\ndata:\ncompartmentocid: b2NpZDIuY29tcGFydG1lbnQub2MyLi5hYWFhYWFhYWEycnNmdmx2eGMzNG8wNjBrZmR5Z3NkczIxbnNrZTc2a3Nqa2tvMjFscHNkZnNm    keyfingerprint: YWI6Y2Q6M2Y6MzQ6MzM6MjI6MzI6MzQ6NTQ6NTQ6NDU6NzY6NzY6Nzg6OTg6YWE=\nprivatekey: &gt;-\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\ntenancyocid: b2NpZDIudGVuYW5jeS5vYzIuLmFhYWFhYWFheXJzZnZsdnhjMzRvMDYwa2ZkeWdzZHMyMW5za2U3NmtzamtrbzIxbHBzZGZzZnNnYnJ0Z2hz\nuserocid: b2NpZDIudXNlci5vYzIuLmFhYWFhYWFhYWFueXJzZnZsdnhjMzRvMDYwa2ZkeWdzZHMyMW5za2U3NmtzamtrbzIxbHBzZGZzZg==\ntype: Opaque\n</code></pre>"},{"location":"input-manifest/providers/oci/#create-oci-credentials","title":"Create OCI credentials","text":""},{"location":"input-manifest/providers/oci/#prerequisites","title":"Prerequisites","text":"<ol> <li>Install OCI CLI by following this guide.</li> <li>Configure OCI CLI by following this guide.</li> </ol>"},{"location":"input-manifest/providers/oci/#creating-oci-credentials-for-claudie","title":"Creating OCI credentials for Claudie","text":"<ol> <li> <p>Export your tenant id: <pre><code>export tenancy_ocid=\"ocid\"\n</code></pre></p> <p>Find your tenant id</p> <p>You can find it under <code>Identity &amp; Security</code> tab and <code>Compartments</code> option.</p> </li> <li> <p>Create OCI compartment where Claudie deploys its resources: <pre><code>{\noci iam compartment create --name claudie-compartment --description claudie-compartment --compartment-id $tenancy_ocid\n}\n</code></pre></p> </li> <li> <p>Create the claudie user: <pre><code>oci iam user create --name claudie-user --compartment-id $tenancy_ocid --description claudie-user --email &lt;email address&gt;\n</code></pre></p> </li> <li> <p>Create a group that will hold permissions for the user: <pre><code>oci iam group create --name claudie-group --compartment-id $tenancy_ocid --description claudie-group\n</code></pre></p> </li> <li> <p>Generate policy file with necessary permissions: <pre><code>{\ncat &gt; policy.txt &lt;&lt;EOF\n[\n  \"Allow group claudie-group to manage instance-family in compartment claudie-compartment\",\n  \"Allow group claudie-group to manage volume-family in compartment claudie-compartment\",\n  \"Allow group claudie-group to manage virtual-network-family in tenancy\",\n  \"Allow group claudie-group to manage dns-zones in compartment claudie-compartment\",\n  \"Allow group claudie-group to manage dns-records in compartment claudie-compartment\"\n]\nEOF\n}\n</code></pre></p> </li> <li> <p>Create a policy with required permissions: <pre><code>oci iam policy create --name claudie-policy --statements file://policy.txt --compartment-id $tenancy_ocid --description claudie-policy\n</code></pre></p> </li> <li> <p>Declare <code>user_ocid</code> and <code>group_ocid</code>: <pre><code>{\ngroup_ocid=$(oci iam group list | jq -r '.data[] | select(.name == \"claudie-group\") | .id')\nuser_ocid=$(oci iam user list | jq -r '.data[] | select(.name == \"claudie-user\") | .id')\n}\n</code></pre></p> </li> <li> <p>Attach claudie-user to claudie-group: <pre><code>oci iam group add-user --group-id $group_ocid --user-id $user_ocid\n</code></pre></p> </li> <li> <p>Generate key pair for claudie-user and enter <code>N/A</code> for no passphrase: <pre><code>oci setup keys --key-name claudie-user --output-dir .\n</code></pre></p> </li> <li> <p>Upload the public key to use for the claudie-user: <pre><code>oci iam user api-key upload --user-id $user_ocid --key-file claudie-user_public.pem\n</code></pre></p> </li> <li> <p>Export <code>compartment_ocid</code> and <code>fingerprint</code>, to use them when creating provider secret. <pre><code>  compartment_ocid=$(oci iam compartment list | jq -r '.data[] | select(.name == \"claudie-compartment\") | .id')\nfingerprint=$(oci iam user api-key list --user-id $user_ocid | jq -r '.data[0].fingerprint')\n</code></pre></p> </li> </ol>"},{"location":"input-manifest/providers/oci/#dns-setup","title":"DNS setup","text":"<p>If you wish to use OCI as your DNS provider where Claudie creates DNS records pointing to Claudie managed clusters, you will need to create a public DNS zone by following this guide.</p> <p>OCI is not my domain registrar</p> <p>You cannot buy a domain from Oracle at this time so you can update nameservers of your OCI hosted zone by following this guide on changing nameservers. However, if you prefer not to use the entire domain, an alternative option is to delegate a subdomain to OCI.</p>"},{"location":"input-manifest/providers/oci/#iam-policies-required-by-claudie","title":"IAM policies required by Claudie","text":"<pre><code>\"Allow group &lt;GROUP_NAME&gt; to manage instance-family in compartment &lt;COMPARTMENT_NAME&gt;\"\n\"Allow group &lt;GROUP_NAME&gt; to manage volume-family in compartment &lt;COMPARTMENT_NAME&gt;\"\n\"Allow group &lt;GROUP_NAME&gt; to manage virtual-network-family in tenancy\"\n\"Allow group &lt;GROUP_NAME&gt; to manage dns-zones in compartment &lt;COMPARTMENT_NAME&gt;\",\n\"Allow group &lt;GROUP_NAME&gt; to manage dns-records in compartment &lt;COMPARTMENT_NAME&gt;\",\n</code></pre>"},{"location":"input-manifest/providers/oci/#input-manifest-examples","title":"Input manifest examples","text":""},{"location":"input-manifest/providers/oci/#single-provider-multi-region-cluster-example","title":"Single provider, multi region cluster example","text":""},{"location":"input-manifest/providers/oci/#create-a-secret-for-oci-provider","title":"Create a secret for OCI provider","text":"<p>The secret for an OCI provider must include the following mandatory fields: <code>compartmentocid</code>, <code>userocid</code>, <code>tenancyocid</code>, <code>keyfingerprint</code> and <code>privatekey</code>.</p> <pre><code># Refer to values exported in \"Creating OCI credentials for Claudie\" section\nkubectl create secret generic oci-secret-1 --namespace=mynamespace --from-literal=compartmentocid=$compartment_ocid --from-literal=userocid=$user_ocid --from-literal=tenancyocid=$tenancy_ocid --from-literal=keyfingerprint=$fingerprint --from-file=privatekey=./claudie-user_public.pem\n</code></pre> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: OCIExampleManifest\nspec:\nproviders:\n- name: oci-1\nproviderType: oci\nsecretRef:\nname: oci-secret-1\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-oci\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-milan-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 1\n# VM shape name.\nserverType: VM.Standard2.2\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: compute-1-oci\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-1\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\n- name: compute-2-oci\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-2\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: oci-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-oci\ncompute:\n- compute-1-oci\n- compute-2-oci\n</code></pre>"},{"location":"input-manifest/providers/oci/#multi-provider-multi-region-clusters-example","title":"Multi provider, multi region clusters example","text":""},{"location":"input-manifest/providers/oci/#create-a-secret-for-oci-provider_1","title":"Create a secret for OCI provider","text":"<p>The secret for an OCI provider must include the following mandatory fields: <code>compartmentocid</code>, <code>userocid</code>, <code>tenancyocid</code>, <code>keyfingerprint</code> and <code>privatekey</code>.</p> <pre><code># Refer to values exported in \"Creating OCI credentials for Claudie\" section\nkubectl create secret generic oci-secret-1 --namespace=mynamespace --from-literal=compartmentocid=$compartment_ocid --from-literal=userocid=$user_ocid --from-literal=tenancyocid=$tenancy_ocid --from-literal=keyfingerprint=$fingerprint --from-file=privatekey=./claudie-user_public.pem\n\nkubectl create secret generic oci-secret-1 --namespace=mynamespace --from-literal=compartmentocid=$compartment_ocid2 --from-literal=userocid=$user_ocid2 --from-literal=tenancyocid=$tenancy_ocid2 --from-literal=keyfingerprint=$fingerprint2 --from-file=privatekey=./claudie-user_public2.pem\n</code></pre> <pre><code>apiVersion: claudie.io/v1beta1\nkind: InputManifest\nmetadata:\nname: OCIExampleManifest\nspec:\nproviders:\n- name: oci-1\nproviderType: oci\nsecretRef:\nname: oci-secret-1\nnamespace: mynamespace\n- name: oci-2\nproviderType: oci\nsecretRef:\nname: oci-secret-2\nnamespace: mynamespace\n\nnodePools:\ndynamic:\n- name: control-oci-1\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-milan-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 1\n# VM shape name.\nserverType: VM.Standard2.2\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: control-oci-2\nproviderSpec:\n# Name of the provider instance.\nname: oci-2\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-3\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\n\n- name: compute-oci-1\nproviderSpec:\n# Name of the provider instance.\nname: oci-1\n# Region of the nodepool.\nregion: eu-frankfurt-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-FRANKFURT-1-AD-1\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region.\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\n- name: compute-oci-2\nproviderSpec:\n# Name of the provider instance.\nname: oci-2\n# Region of the nodepool.\nregion: eu-milan-1\n# Availability domain of the nodepool.\nzone: hsVQ:EU-MILAN-1-AD-1\ncount: 2\n# VM shape name.\nserverType: VM.Standard2.1\n# OCID of the image.\n# Make sure to update it according to the region..\nimage: ocid1.image.oc1.eu-frankfurt-1.aaaaaaaavvsjwcjstxt4sb25na65yx6i34bzdy5oess3pkgwyfa4hxmzpqeq\nstorageDiskSize: 50\n\nkubernetes:\nclusters:\n- name: oci-cluster\nversion: v1.24.0\nnetwork: 192.168.2.0/24\npools:\ncontrol:\n- control-oci-1\n- control-oci-2\ncompute:\n- compute-oci-1\n- compute-oci-2\n</code></pre>"},{"location":"loadbalancing/loadbalancing-solution/","title":"Claudie load balancing solution","text":""},{"location":"loadbalancing/loadbalancing-solution/#loadbalancer","title":"Loadbalancer","text":"<p>To create a highly available kubernetes cluster, Claudie creates load balancers for the <code>kubeAPI</code> server. These load balancers use Nginx to load balance the traffic among the cluster nodes. Claudie also supports definition of custom load balancers for the applications running inside the cluster.</p>"},{"location":"loadbalancing/loadbalancing-solution/#concept","title":"Concept","text":"<ul> <li> <p>The load balancer machines will join the Wireguard private network of Claudie clusters relevant to it.</p> <ul> <li>This is necessary so that the LB machines can send traffic to the cluster machines over the <code>wireguard VPN</code>.</li> </ul> </li> <li> <p>DNS A records will be created and managed by Claudie on 1 or more cloud providers.</p> <ul> <li>There will be a DNS A record for the public IP of each LB machine that is currently passing the health checks.</li> </ul> </li> <li> <p>The LB machines will run an <code>Nginx</code> to carry out the actual load balancing.</p> <ul> <li>There will be a DNS A record for the public IP of each LB machine that is currently passing the health checks.</li> <li>Therefore, there will be actually 2 layers of load balancing.<ol> <li>DNS-based load balancing to determine the LB machine to be used.</li> <li>Software load balancing on the chosen LB machine.</li> </ol> </li> </ul> </li> <li> <p>Claudie will dynamically manage the LB configuration, e.g. if some cluster node is removed, the LB configuration changes or DNS configuration changes (hostname change).</p> </li> <li> <p>The load balancing will be on L4 layer, TCP/UDP, partially configurable by the Claudie input manifest.</p> </li> </ul>"},{"location":"loadbalancing/loadbalancing-solution/#example-diagram","title":"Example diagram","text":""},{"location":"loadbalancing/loadbalancing-solution/#definitions","title":"Definitions","text":""},{"location":"loadbalancing/loadbalancing-solution/#role","title":"Role","text":"<p>Claudie uses the concept of roles while configuring the load balancers from the input manifest. Each role represents a loadbalancer configuration for a particular use. Roles are then assigned to the load balancer cluster. A single load balancer cluster can have multiple roles assigned.</p>"},{"location":"loadbalancing/loadbalancing-solution/#targeted-kubernetes-cluster","title":"Targeted kubernetes cluster","text":"<p>Load balancer gets assigned to a kubernetes cluster with the field <code>targetedK8s</code>. This field is using the <code>name</code> of the kubernetes cluster as a value. Currently, a single load balancer can only be assigned to a single kubernetes cluster.</p> <p>Among multiple load balancers targeting the same kubernetes cluster only one of them can have the API server role (i.e. the role with target port 6443) attached to it.</p>"},{"location":"loadbalancing/loadbalancing-solution/#dns","title":"DNS","text":"<p>Claudie creates and manages the DNS for the load balancer. If the user adds a load balancer into their infrastructure via Claudie, Claudie creates a DNS A record with the public IP of the load balancer machines behind it. When the load balancer configuration changes in any way, that is a node is added/removed, the hostname or the target changes, the DNS record is reconfigured by Claudie on the fly. This rids the user of the need to manage DNS.</p>"},{"location":"loadbalancing/loadbalancing-solution/#nodepools","title":"Nodepools","text":"<p>Loadbalancers are build from user defined nodepools in <code>pools</code> field, similar to how kubernetes clusters are defined. These nodepools allow the user to change/scale the load balancers according to their needs without any fuss. See the nodepool definition for more information.</p>"},{"location":"loadbalancing/loadbalancing-solution/#an-example-of-load-balancer-definition","title":"An example of load balancer definition","text":"<p>See an example load balancer definition in our reference example input manifest.</p>"},{"location":"roadmap/roadmap/","title":"Roadmap for Claudie","text":"<ul> <li> Support for more cloud providers<ul> <li> OCI</li> <li> AWS</li> <li> Azure</li> <li> Cloudflare</li> </ul> </li> <li> Periodic health checks &amp; sync loops for the managed clusters</li> <li> Hybrid-cloud support (on-premises)</li> <li> <code>arm64</code> support for the nodepools</li> <li> Support for Spot instances</li> <li> Autoscaler</li> </ul>"},{"location":"storage/storage-solution/","title":"Claudie storage solution","text":""},{"location":"storage/storage-solution/#concept","title":"Concept","text":"<p>Running stateful workloads is a complex task, even more so when considering the multi-cloud environment. Claudie therefore needs to be able to accommodate stateful workloads, regardless of the underlying infrastructure providers.</p> <p>Claudie orchestrates storage on the kubernetes cluster nodes by creating one \"storage cluster\" across multiple providers. This \"storage cluster\" has a series of <code>zones</code>, one for each cloud provider instance. Each <code>zone</code> then stores its own persistent volume data.</p> <p>This concept is translated into longhorn implementation, where each <code>zone</code> is represented by a Storage Class which is backed up by the nodes defined under the same cloud provider instance. Furthermore, each node uses separate disk to the one, where OS is installed, to assure clear data separation. The size of the storage disk can be configured in <code>storageDiskSize</code> field of the nodepool specification.</p>"},{"location":"storage/storage-solution/#longhorn","title":"Longhorn","text":"<p>A Claudie-created cluster comes with the <code>longhorn</code> deployment preinstalled and ready to be used. By default, only worker nodes are used to store data.</p> <p>Longhorn installed in the cluster is set up in a way that it provides one default <code>StorageClass</code> called <code>longhorn</code>, which, if used, creates a volume that is then replicated across random nodes in the cluster.</p> <p>Besides the default storage class, Claudie can also create custom storage classes, which force persistent volumes to be created on specific nodes based on the provider instance they have. In other words, you can use a specific provider instance to provision nodes for your storage needs, while using another provider instance for computing tasks.</p>"},{"location":"storage/storage-solution/#example","title":"Example","text":"<p>To follow along, have a look at the reference example input manifest file.</p> <p>When Claudie applies this input manifest, the following storage classes are installed:</p> <ul> <li><code>longhorn</code> - the default storage class, which stores data on random nodes</li> <li><code>longhorn-&lt;provider instance&gt;-zone</code> - storage class, which stores data only on nodes of the specific provider instance (see the list of supported providers), i.e. <code>longhorn-gcp-1-zone</code>, <code>longhorn-gcp-2-zone</code>, <code>longhorn-aws-1-zone</code>, ...</li> </ul> <p>For more information on how Longhorn works you can check out Longhorn's official documentation.</p>"},{"location":"troubleshooting/troubleshooting/","title":"Troubleshooting guide","text":"<p>In progress</p> <p>As we continue working on our troubleshooting guide, we understand that issues may arise during your usage of Claudie. Although the guide is not yet complete, we encourage you to create a GitHub issue if you encounter any problems. Rest assured that we are committed to promptly responding to your concerns and providing assistance, ensuring a smooth experience even before the troubleshooting guide is finalized. Your feedback and reports are highly valuable to us in improving our platform and addressing any issues you may face.</p>"},{"location":"use-cases/use-cases/","title":"Use-cases and customers","text":"<p>We foresee the following use-cases of the Claudie platform</p>"},{"location":"use-cases/use-cases/#1-cloud-bursting","title":"1. Cloud-bursting","text":"<p>A company uses advanced cloud features in one of the hyper-scale providers (e.g. serverless Lambda and API Gateway functionality in AWS). They run a machine-learning application that they need to train for a pattern on a dataset. The learning phase requires significant compute resources. Claudie allows to extend the cluster in AWS (needed in order to access the AWS functionality) to Hetzner for saving the infrastructure costs of the machine-learning case.</p> <p>Typical client profiles:</p> <ul> <li>startups</li> <li>in need of significant computing power already in their early stages (e.g. AI/ML workloads)</li> </ul>"},{"location":"use-cases/use-cases/#2-cost-saving","title":"2. Cost-saving","text":"<p>A company would like to utilize their on-premise or leased resources that they already invested into, but would like to:</p> <ol> <li>extend the capacity</li> <li>access managed features of a hyper-scale provider (AWS, GCP, ...)</li> <li>get the workload physically closer to a client (e. g. to South America)</li> </ol> <p>Typical client profile:</p> <ul> <li>medium-size business</li> <li>possibly already familiar with containerized workload</li> </ul>"},{"location":"use-cases/use-cases/#3-smart-layer-as-a-service-on-top-of-simple-cloud-providers","title":"3. Smart-layer-as-a-Service on top of simple cloud-providers","text":"<p>An existing customer of medium-size provider (e.g. Exoscale) would like to utilize features that are typical for hyper-scale providers. Their current provider does neither offer nor plan to offer such an advanced functionality.</p> <p>Typical client profile:</p> <ul> <li>established business</li> <li>need to access advanced managed features to innovate faster</li> </ul>"},{"location":"use-cases/use-cases/#4-service-interconnect","title":"4. Service interconnect","text":"<p>A company would like to access on-premise-hosted services and cloud-managed services from within the same cluster. For on-premise services the on-premise cluster node would egress the traffic. The cloud-hosted cluster nodes would deal with the egress traffic to the cloud-managed services.</p> <p>Typical client profile:</p> <ul> <li>medium-size/established business</li> <li>already contains on-premise workloads</li> <li>has the need to take the advantage of managed cloud infra (from cost, agility, or capacity reasons)</li> </ul>"}]}